{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMERuecZBNSnYJic0EZm7tb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukeSchmitt96/gym-pybullet-drones/blob/master/tether_sim/DDPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh6uP-U8pcyu"
      },
      "source": [
        "!git clone https://github.com/LukeSchmitt96/gym-pybullet-drones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XU_WJWWpm6Z"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsapDGk0poy8"
      },
      "source": [
        "pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1HSfwubprdN"
      },
      "source": [
        "pip install stable-baselines3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glYLGhqjptC9"
      },
      "source": [
        "pip install 'ray[rllib]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAkoscpspvoB"
      },
      "source": [
        "cd gym-pybullet-drones/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JlgFgSupxVH"
      },
      "source": [
        "pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3dp_IXapyyQ"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import pdb\n",
        "import math\n",
        "import numpy as np\n",
        "import pybullet as p\n",
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines3.ddpg.policies import MlpPolicy\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "from gym_pybullet_drones.envs.RLTetherAviary import RLTetherAviary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    #### Create custom policy ##########################################################################\n",
        "    CustomPolicy = MlpPolicy\n",
        "    CustomPolicy.layers = [80,80,32]    # actor network has layers [80, 80, 32]\n",
        "\n",
        "    #### Check the environment's spaces ################################################################\n",
        "    env = RLTetherAviary(gui=0, record=0)\n",
        "    print(\"[INFO] Action space:\", env.action_space)\n",
        "    print(\"[INFO] Observation space:\", env.observation_space)\n",
        "\n",
        "    #### Train the model ###############################################################################\n",
        "    n_actions = env.action_space.shape[-1]\n",
        "    model = DDPG(MlpPolicy, env, verbose=1, batch_size=64)\n",
        "    \n",
        "    training_timesteps = 200000\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        # print(\"Iteration\\t\", i)\n",
        "\n",
        "        model.learn(total_timesteps=training_timesteps)\n",
        "        model.save(\"ddpg\"+str((i+1)*training_timesteps))\n",
        "        model.save_replay_buffer(\"ddpg_experience\"+str((i+1)*training_timesteps))\n",
        "\n",
        "        #### Show (and record a video of) the model's performance ##########################################\n",
        "        env_test = RLTetherAviary(gui=False, record=False)\n",
        "        obs = env_test.reset()\n",
        "        start = time.time()\n",
        "        for i in range(10*env_test.SIM_FREQ):\n",
        "            action, _states = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, info = env_test.step(action)\n",
        "            print(i)\n",
        "            print(obs)\n",
        "            print(done)\n",
        "            env_test.render()\n",
        "            if done: break\n",
        "        env_test.close()\n",
        "\n",
        "    env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmOn8PnqqDVM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}