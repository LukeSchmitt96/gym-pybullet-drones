{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_Action_Array.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOP0Jcmz2nPO3kA5Wv/Sx9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukeSchmitt96/gym-pybullet-drones/blob/master/tether_sim/DQN_Action_Array.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjhCsjmOPVUV"
      },
      "source": [
        "Runtime>Change runtime type>Hardware accelerator: Change it to 'GPU'>SAVE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYV4bFKuP3c8"
      },
      "source": [
        "Google Colab automatically closes after 12 hours. Save files before that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg1kmS5VP-YE"
      },
      "source": [
        "Use autoclicker: https://sourceforge.net/projects/orphamielautoclicker/files/AutoClicker.exe/download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmI6TI83PWaN"
      },
      "source": [
        "!git clone https://github.com/LukeSchmitt96/gym-pybullet-drones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6U2OidYOFVv"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOgU0XifOLuJ"
      },
      "source": [
        "pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BRce4U9ONML"
      },
      "source": [
        "pip install stable-baselines3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URW5PoIiOOwu"
      },
      "source": [
        "pip install 'ray[rllib]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52o0Y1MjNpwU"
      },
      "source": [
        "cd gym-pybullet-drones/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrlGJLNcN9rg"
      },
      "source": [
        "pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4XOplT3tNjDU",
        "outputId": "3c1f6a22-73d2-47cf-f53e-8a6ff8902b7e"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import keras, tensorflow as tf, numpy as np, gym, sys, copy, argparse\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import model_from_json\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from gym_pybullet_drones.envs.RLTetherAviary import RLTetherAviary\n",
        "# from gym_pybullet_drones.envs.RLCrazyFlieAviary import RLCrazyFlieAviary\n",
        "\n",
        "class DQN():\n",
        "\n",
        "\tdef __init__(self, environment):\n",
        "\t\t\n",
        "\t\tself.environment = environment\n",
        "\t\tself.num_actions = self.environment.action_space.shape[0] # Pitch forward, Pitch backward, Roll left, Roll right, Hover\n",
        "\t\tself.state_size = self.environment.reset().shape[0]\n",
        "\t\tself.model = Sequential()\n",
        "\t\tself.model.add(Dense(128, input_dim=self.state_size, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\t# self.model.add(Dense(64, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\tself.model.add(Dense(64, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\tself.model.add(Dense(32, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\tself.model.add(Dense(self.num_actions))\n",
        "\t\tself.model.compile(loss = 'mse', optimizer = Adam(lr = 0.001)) # Define loss and learning rate\n",
        "\n",
        "\tdef save_model_weights(self):\n",
        "\t\t\n",
        "\t\tmodel_json = self.model.to_json()\n",
        "\t\twith open(\"model.json\", \"w\") as json_file:\n",
        "\t\t\tjson_file.write(model_json)\n",
        "\t\tself.model.save_weights(\"model.h5\")\n",
        "\n",
        "\tdef load_model(self, model_file):\n",
        "\t\t\n",
        "\t\tjson_file = open('model.json', 'r')\n",
        "\t\tloaded_model_json = json_file.read()\n",
        "\t\tjson_file.close()\n",
        "\t\tself.model = model_from_json(loaded_model_json)\n",
        "\n",
        "\tdef load_model_weights(self, weight_file):\n",
        "\t\t\n",
        "\t\tself.model.load_weights(\"model.h5\")\n",
        "\n",
        "\n",
        "class Replay_Memory():\n",
        "\n",
        "\tdef __init__(self, memory_size = 50000): # Define replay memory size\n",
        "\t\t\n",
        "\t \tself.memory = collections.deque(maxlen = memory_size)\n",
        "\n",
        "\tdef sample_batch(self, batch_size = 32): # Define batch size\n",
        "\t\t\n",
        "\t\tif len(self.memory) < batch_size:\n",
        "\t\t\treturn None\n",
        "\t\telse:\n",
        "\t\t\treturn random.sample(self.memory, batch_size)\n",
        "\n",
        "\tdef append(self, transition):\n",
        "\t\t\n",
        "\t\tself.memory.append(transition)\n",
        "\n",
        "class DQN_Agent():\n",
        "\n",
        "\tdef __init__(self, environment_name):\n",
        "\n",
        "\t\t# self.environment_name = environment_name\n",
        "\t\tself.env = environment_name # gym.envs.make(self.environment_name)\n",
        "\t\tself.train_network = DQN(self.env)\n",
        "\t\tself.target_network = DQN(self.env)\n",
        "\t\tself.target_network.model.set_weights(self.train_network.model.get_weights())\n",
        "\t\tself.memory = Replay_Memory()\n",
        "\t\tself.num_episodes = 10000 # Define number of episodes\n",
        "\t\tself.gamma = 0.99 # Define discount rate\n",
        "\t\tself.eps_limit = 0.5 # Define Epsilon\n",
        "\t\tself.target_update = 5 # Define target net update frequency\n",
        "\n",
        "\tdef index_to_array(self, index):\n",
        "\n",
        "\t\t# Hover RPM = 14468 Max RPM = 21702\n",
        "\t\ttether_force = 0 # Define tether force\n",
        "\t\tmin = 0.6 # Define RPM\n",
        "\t\tmax = 0.7 # Define RPM\n",
        "\t\thov = 0 # Define Hover RPM\n",
        "\n",
        "\t\tif index == 0:\n",
        "\t\t\taction_array = np.array([min, min, max, max, tether_force]) # Rotor 3 and Rotor 4, X-axis\n",
        "\t\telif index == 1:\n",
        "\t\t\taction_array = np.array([max, max, min, min, tether_force]) # Rotor 1 and Rotor 2, X-axis\n",
        "\t\telif index == 2:\n",
        "\t\t\taction_array = np.array([max, min, max, min, tether_force]) # Rotor 1 and Rotor 3, Y-axis\n",
        "\t\telif index == 3:\n",
        "\t\t\taction_array = np.array([min, max, min, max, tether_force]) # Rotor 2 and Rotor 4, Y-axis\n",
        "\t\telif index == 4:\n",
        "\t\t\taction_array = np.array([hov, hov, hov, hov, 0]) # Hover\n",
        "\t\t\n",
        "\t\treturn action_array\n",
        "\n",
        "\tdef epsilon_greedy_policy(self, q_values):\n",
        "\t\t\n",
        "\t\teps = np.random.random(1)[0]\n",
        "\t\tif eps < self.eps_limit:\n",
        "\t\t\taction_index = np.random.randint(0, self.train_network.num_actions)\n",
        "\t\n",
        "\t\telse:\n",
        "\t\t\taction_index = np.argmax(q_values)\n",
        "\t\t\n",
        "\t\taction = self.index_to_array(action_index)\n",
        "\n",
        "\t\tcurrent_q = q_values[action_index]\n",
        "\t\t\n",
        "\t\treturn action_index, action, current_q\n",
        "\n",
        "\tdef greedy_policy(self, q_values):\n",
        "\t\t\n",
        "\t\taction = np.argmax(q_values)\n",
        "\n",
        "\t\treturn action\n",
        "\t\n",
        "\tdef update_model(self):\n",
        "\t\n",
        "\t\tstates = []\n",
        "\t\tobs_states = []\n",
        "\t\tbatch = self.memory.sample_batch()\n",
        "\n",
        "\t\tif batch == None:\n",
        "\t\t\treturn\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tfor tup in batch:\n",
        "\t\t\t\tstate, action_index, reward, obs, done = tup\n",
        "\t\t\t\tstates.append(state)\n",
        "\t\t\t\tobs_states.append(obs)\n",
        "\t\t\t\n",
        "\t\t\tstates = np.asarray(states)\n",
        "\t\t\tobs_states = np.asarray(obs_states)\n",
        "\n",
        "\t\t\ttarget_q = (self.train_network.model.predict(states))\n",
        "\t\t\ttarget_q_obs = (self.target_network.model.predict(obs_states))\n",
        "\t\t\t\t\n",
        "\t\t\ti = 0\t\n",
        "\t\t\tfor tup in batch:\n",
        "\t\t\t\tstate, action_index, reward, obs, done = tup\n",
        "\t\t\t\tif done:\n",
        "\t\t\t\t\ttarget_q[i][action_index] = reward\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tq_s = max(target_q_obs[i])\n",
        "\t\t\t\t\ttarget_q[i][action_index] = reward + q_s * self.gamma\n",
        "\t\t\t\ti += 1\n",
        "\n",
        "\t\t\tself.train_network.model.train_on_batch(states, target_q) ##### Backprop qvalues\n",
        "\n",
        "\tdef train(self):\n",
        "\t\t\n",
        "\t\tself.test_list = []\n",
        "\t\tself.train_td = []\n",
        "\t\tepisode = 0\n",
        "\t\twhile episode < self.num_episodes:\n",
        "\t\t\tdone = False\n",
        "\t\t\tstate = self.env.reset()\n",
        "\t\t\tepisode_td = []\n",
        "\t\t\tepisode_reward = 0\n",
        "\t\t\twhile not done:\n",
        "\t\t\t\tq_values = (self.train_network.model.predict(state[None, :]))[0]\n",
        "\t\t\t\taction_index, action, q = self.epsilon_greedy_policy(q_values)\n",
        "\t\t\t\tprint(action_index)\n",
        "\t\t\t\tobs, reward, done,  _ = self.env.step(action)\n",
        "\t\t\t\tepisode_reward += reward\n",
        "\t\t\t\t\n",
        "\t\t\t\ttd_step = abs(reward + self.gamma * np.max((self.target_network.model.predict(obs[None, :]))) - q)\n",
        "\t\t\t\tepisode_td.append(td_step)\n",
        "\t\t\t\tself.memory.append((state, action_index, reward, obs, done))\n",
        "\t\t\t\tself.update_model()\n",
        "\t\t\t\tstate = obs\n",
        "\t\t\t\n",
        "\t\t\tself.train_td.append(np.mean(np.array(episode_td)))\n",
        "\n",
        "\t\t\tprint('Episode: ', episode)\n",
        "\t\t\tprint('Current Train Reward: ', episode_reward)\n",
        "\t\t\tprint('Current TD Error: ', self.train_td[-1])\n",
        "\t\t\tprint('\\n')\n",
        "\n",
        "\t\t\tif episode % self.target_update == 0:\n",
        "\t\t\t\tself.target_network.model.set_weights(self.train_network.model.get_weights())\n",
        "\t\t\t\n",
        "\t\t\tif episode > 50:\n",
        "\t\t\t\tself.eps_limit = max(self.eps_limit - 0.005, 0.01) # Define exploration decay rate\n",
        "\t\t\t\n",
        "\t\t\tif episode % 100 == 0:\n",
        "\t\t\t\tself.train_network.save_model_weights()\n",
        "\t\t\t\n",
        "\t\t\tif episode % 100 == 0:\n",
        "\t\t\t\tcurrent_test_reward = self.test(20)\n",
        "\t\t\t\tself.test_list.append(current_test_reward)\n",
        "\t\t\t\tprint('Current Test Reward: ', current_test_reward)\n",
        "\t\t\t\tnp.savetxt('td_error.txt', self.train_td)\n",
        "\t\t\t\tnp.savetxt('test_list.txt', self.test_list)\n",
        "\t\t\t\t\n",
        "\t\t\t\tif current_test_reward >= 1000:\n",
        "\t\t\t\t\tprint('Problem solved!')\n",
        "\t\t\t\t\tself.train_network.save_model_weights()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tself.num_episodes = episode + 2\n",
        "\n",
        "\t\t\tepisode += 1\n",
        "\t\t\n",
        "\t\tself.test_reward = self.test_list\n",
        "\t\t\n",
        "\t\tnp.savetxt('td.txt', self.train_td)\n",
        "\t\tnp.savetxt('reward.txt', self.test_reward)\n",
        "\t\tplt.figure(figsize = (12, 8))\n",
        "\t\tplt.plot(np.arange(len(self.test_reward)) * 100, np.ones(len(self.test_reward)) * 200, label = 'Target reward')\n",
        "\t\tplt.plot(np.arange(len(self.test_reward)) * 100, self.test_reward, label = 'Mean reward')\n",
        "\t\tplt.grid()\n",
        "\t\tplt.xlabel('Number of episodes', fontsize = 18)\n",
        "\t\tplt.ylabel('Cumulative reward', fontsize = 18)\n",
        "\t\tplt.legend(fontsize = 18)\n",
        "\t\tplt.show()\n",
        "\t\t\n",
        "\t\tplt.figure(figsize = (12, 8))\n",
        "\t\tplt.plot(np.arange(len(self.train_td)) * 100, self.train_td, label = 'Train TD Error')\n",
        "\t\tplt.grid()\n",
        "\t\tplt.xlabel('Number of Training Episodes', fontsize = 18)\n",
        "\t\tplt.ylabel('Average TD Error', fontsize = 18)\n",
        "\t\tplt.legend(fontsize = 18)\n",
        "\t\tplt.show()\n",
        "\n",
        "\tdef test(self, episodes):\n",
        "\t\t\n",
        "\t\ttest_reward = []\n",
        "\t\tfor i in range(episodes):\n",
        "\t\t\tdone = False\n",
        "\t\t\tstate = self.env.reset()\n",
        "\t\t\tepisode_reward = 0\n",
        "\t\t\twhile not done:\n",
        "\t\t\t\taction_index = np.argmax(self.target_network.model.predict(state[None, :]))\n",
        "\t\t\t\taction = self.index_to_array(action_index)\n",
        "\t\t\t\tobs, reward, done, _ = self.env.step(action)\n",
        "\t\t\t\tepisode_reward += reward\n",
        "\t\t\t\tstate = obs\n",
        "\t\t\ttest_reward.append(episode_reward)\n",
        "\n",
        "\t\treturn np.mean(test_reward)\n",
        "\n",
        "def main():\n",
        "\n",
        "\tMODEL = DQN_Agent(RLTetherAviary(gui = 1, record = 0))\n",
        "\t# MODEL.train_network.load_model_weights('model.h5')\n",
        "\tMODEL.train()\n",
        "\t\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
            "[INFO] m 0.027000, L 0.039700,\n",
            "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
            "[INFO] kf 0.000000, km 0.000000,\n",
            "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
            "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
            "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
            "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode:  0\n",
            "Current Train Reward:  -1077.0775088401178\n",
            "Current TD Error:  12.75783417257492\n",
            "\n",
            "\n",
            "Current Test Reward:  -1086.3048002889075\n",
            "Episode:  1\n",
            "Current Train Reward:  -1094.6482414837915\n",
            "Current TD Error:  12.127193701417061\n",
            "\n",
            "\n",
            "Episode:  2\n",
            "Current Train Reward:  -1100.1194302352148\n",
            "Current TD Error:  12.335134269545222\n",
            "\n",
            "\n",
            "Episode:  3\n",
            "Current Train Reward:  -1065.170040198275\n",
            "Current TD Error:  19.938664788683333\n",
            "\n",
            "\n",
            "Episode:  4\n",
            "Current Train Reward:  -1073.5389202308045\n",
            "Current TD Error:  15.5419809408441\n",
            "\n",
            "\n",
            "Episode:  5\n",
            "Current Train Reward:  -1050.1921779893073\n",
            "Current TD Error:  15.544261710300944\n",
            "\n",
            "\n",
            "Episode:  6\n",
            "Current Train Reward:  -1087.1162386795745\n",
            "Current TD Error:  18.85386779811405\n",
            "\n",
            "\n",
            "Episode:  7\n",
            "Current Train Reward:  -1103.5903636746814\n",
            "Current TD Error:  18.529776037225172\n",
            "\n",
            "\n",
            "Episode:  8\n",
            "Current Train Reward:  -1058.2218347505989\n",
            "Current TD Error:  16.949459550209056\n",
            "\n",
            "\n",
            "Episode:  9\n",
            "Current Train Reward:  -1104.8326018308096\n",
            "Current TD Error:  17.785918143796454\n",
            "\n",
            "\n",
            "Episode:  10\n",
            "Current Train Reward:  -1047.6332958466537\n",
            "Current TD Error:  20.108844144928316\n",
            "\n",
            "\n",
            "Episode:  11\n",
            "Current Train Reward:  -1078.7370867337281\n",
            "Current TD Error:  17.756483619779335\n",
            "\n",
            "\n",
            "Episode:  12\n",
            "Current Train Reward:  -1082.5251022524344\n",
            "Current TD Error:  15.969628472249546\n",
            "\n",
            "\n",
            "Episode:  13\n",
            "Current Train Reward:  -1063.8333781305685\n",
            "Current TD Error:  15.40213961129652\n",
            "\n",
            "\n",
            "Episode:  14\n",
            "Current Train Reward:  -1078.4964244581697\n",
            "Current TD Error:  16.632490780602673\n",
            "\n",
            "\n",
            "Episode:  15\n",
            "Current Train Reward:  -1062.0468205631435\n",
            "Current TD Error:  15.89588726097557\n",
            "\n",
            "\n",
            "Episode:  16\n",
            "Current Train Reward:  -1098.3153032828684\n",
            "Current TD Error:  15.129373013104722\n",
            "\n",
            "\n",
            "Episode:  17\n",
            "Current Train Reward:  -1052.0443996693864\n",
            "Current TD Error:  16.414849734269488\n",
            "\n",
            "\n",
            "Episode:  18\n",
            "Current Train Reward:  -1075.3454177837325\n",
            "Current TD Error:  11.481507258046324\n",
            "\n",
            "\n",
            "Episode:  19\n",
            "Current Train Reward:  -1060.6262499966401\n",
            "Current TD Error:  16.878774766083914\n",
            "\n",
            "\n",
            "Episode:  20\n",
            "Current Train Reward:  -1064.8902095148117\n",
            "Current TD Error:  14.252790969772379\n",
            "\n",
            "\n",
            "Episode:  21\n",
            "Current Train Reward:  -1062.0487757580875\n",
            "Current TD Error:  19.79135228730572\n",
            "\n",
            "\n",
            "Episode:  22\n",
            "Current Train Reward:  -1077.3426161985362\n",
            "Current TD Error:  15.221203727568302\n",
            "\n",
            "\n",
            "Episode:  23\n",
            "Current Train Reward:  -1077.4411035777182\n",
            "Current TD Error:  20.31737643867544\n",
            "\n",
            "\n",
            "Episode:  24\n",
            "Current Train Reward:  -1062.4094826223993\n",
            "Current TD Error:  15.347036174580895\n",
            "\n",
            "\n",
            "Episode:  25\n",
            "Current Train Reward:  -1100.0196901502943\n",
            "Current TD Error:  14.888100344097234\n",
            "\n",
            "\n",
            "Episode:  26\n",
            "Current Train Reward:  -1069.712619693013\n",
            "Current TD Error:  16.753092277254915\n",
            "\n",
            "\n",
            "Episode:  27\n",
            "Current Train Reward:  -1089.4257150941787\n",
            "Current TD Error:  14.009629235007154\n",
            "\n",
            "\n",
            "Episode:  28\n",
            "Current Train Reward:  -1084.1119584324529\n",
            "Current TD Error:  18.621171329217546\n",
            "\n",
            "\n",
            "Episode:  29\n",
            "Current Train Reward:  -1092.732845626423\n",
            "Current TD Error:  15.01537640070428\n",
            "\n",
            "\n",
            "Episode:  30\n",
            "Current Train Reward:  -1082.8101105383805\n",
            "Current TD Error:  13.495531229045719\n",
            "\n",
            "\n",
            "Episode:  31\n",
            "Current Train Reward:  -1078.2967114935623\n",
            "Current TD Error:  12.026277568962708\n",
            "\n",
            "\n",
            "Episode:  32\n",
            "Current Train Reward:  -1062.9655080072976\n",
            "Current TD Error:  12.373775458618317\n",
            "\n",
            "\n",
            "Episode:  33\n",
            "Current Train Reward:  -1059.9265611582764\n",
            "Current TD Error:  14.446765385206618\n",
            "\n",
            "\n",
            "Episode:  34\n",
            "Current Train Reward:  -1055.7157360438678\n",
            "Current TD Error:  15.752923623445444\n",
            "\n",
            "\n",
            "Episode:  35\n",
            "Current Train Reward:  -1105.3699406977344\n",
            "Current TD Error:  13.311377223678239\n",
            "\n",
            "\n",
            "Episode:  36\n",
            "Current Train Reward:  -1057.2177745820775\n",
            "Current TD Error:  15.403172097762592\n",
            "\n",
            "\n",
            "Episode:  37\n",
            "Current Train Reward:  -1099.4080038397906\n",
            "Current TD Error:  12.861448177028738\n",
            "\n",
            "\n",
            "Episode:  38\n",
            "Current Train Reward:  -1082.7663929736404\n",
            "Current TD Error:  13.302738199467816\n",
            "\n",
            "\n",
            "Episode:  39\n",
            "Current Train Reward:  -1067.4456098403348\n",
            "Current TD Error:  14.296449411333489\n",
            "\n",
            "\n",
            "Episode:  40\n",
            "Current Train Reward:  -1078.562310565689\n",
            "Current TD Error:  13.794376981616631\n",
            "\n",
            "\n",
            "Episode:  41\n",
            "Current Train Reward:  -1053.434877768333\n",
            "Current TD Error:  14.604224319189765\n",
            "\n",
            "\n",
            "Episode:  42\n",
            "Current Train Reward:  -1057.9006294839724\n",
            "Current TD Error:  12.940128719649325\n",
            "\n",
            "\n",
            "Episode:  43\n",
            "Current Train Reward:  -1092.3640798343936\n",
            "Current TD Error:  12.805239089596231\n",
            "\n",
            "\n",
            "Episode:  44\n",
            "Current Train Reward:  -1100.6982270534727\n",
            "Current TD Error:  11.71265362145858\n",
            "\n",
            "\n",
            "Episode:  45\n",
            "Current Train Reward:  -1085.0464157807355\n",
            "Current TD Error:  12.551755663951587\n",
            "\n",
            "\n",
            "Episode:  46\n",
            "Current Train Reward:  -1068.7169951207322\n",
            "Current TD Error:  13.950314683545184\n",
            "\n",
            "\n",
            "Episode:  47\n",
            "Current Train Reward:  -1092.5005465366887\n",
            "Current TD Error:  12.837007321826716\n",
            "\n",
            "\n",
            "Episode:  48\n",
            "Current Train Reward:  -1072.8128913863188\n",
            "Current TD Error:  13.39550489822915\n",
            "\n",
            "\n",
            "Episode:  49\n",
            "Current Train Reward:  -1090.2147417911358\n",
            "Current TD Error:  13.353326499257435\n",
            "\n",
            "\n",
            "Episode:  50\n",
            "Current Train Reward:  -1070.5719808794183\n",
            "Current TD Error:  13.91917643730691\n",
            "\n",
            "\n",
            "Episode:  51\n",
            "Current Train Reward:  -1054.5278709518298\n",
            "Current TD Error:  15.288823916137327\n",
            "\n",
            "\n",
            "Episode:  52\n",
            "Current Train Reward:  -1094.8819943329438\n",
            "Current TD Error:  11.131062382431518\n",
            "\n",
            "\n",
            "Episode:  53\n",
            "Current Train Reward:  -1091.0541876371644\n",
            "Current TD Error:  12.206841514974768\n",
            "\n",
            "\n",
            "Episode:  54\n",
            "Current Train Reward:  -1064.816743309013\n",
            "Current TD Error:  11.859297102446705\n",
            "\n",
            "\n",
            "Episode:  55\n",
            "Current Train Reward:  -1056.422035498767\n",
            "Current TD Error:  14.442516650117586\n",
            "\n",
            "\n",
            "Episode:  56\n",
            "Current Train Reward:  -1108.3384137325554\n",
            "Current TD Error:  12.926692851660263\n",
            "\n",
            "\n",
            "Episode:  57\n",
            "Current Train Reward:  -1049.9543583848192\n",
            "Current TD Error:  13.072171471781415\n",
            "\n",
            "\n",
            "Episode:  58\n",
            "Current Train Reward:  -1086.6938143804844\n",
            "Current TD Error:  11.923042754996962\n",
            "\n",
            "\n",
            "Episode:  59\n",
            "Current Train Reward:  -1064.5844460991723\n",
            "Current TD Error:  13.961045920656076\n",
            "\n",
            "\n",
            "Episode:  60\n",
            "Current Train Reward:  -1092.8296666072022\n",
            "Current TD Error:  10.672732698511224\n",
            "\n",
            "\n",
            "Episode:  61\n",
            "Current Train Reward:  -1080.5648838054435\n",
            "Current TD Error:  13.059025026439427\n",
            "\n",
            "\n",
            "Episode:  62\n",
            "Current Train Reward:  -1096.7193374886854\n",
            "Current TD Error:  10.920159829982492\n",
            "\n",
            "\n",
            "Episode:  63\n",
            "Current Train Reward:  -1078.7004318960135\n",
            "Current TD Error:  12.214901834130334\n",
            "\n",
            "\n",
            "Episode:  64\n",
            "Current Train Reward:  -1053.3739986450862\n",
            "Current TD Error:  13.839258750408536\n",
            "\n",
            "\n",
            "Episode:  65\n",
            "Current Train Reward:  -1105.353387086099\n",
            "Current TD Error:  11.058645620057264\n",
            "\n",
            "\n",
            "Episode:  66\n",
            "Current Train Reward:  -1084.2505679206888\n",
            "Current TD Error:  12.707977651450657\n",
            "\n",
            "\n",
            "Episode:  67\n",
            "Current Train Reward:  -1098.7453376413778\n",
            "Current TD Error:  11.92493120467368\n",
            "\n",
            "\n",
            "Episode:  68\n",
            "Current Train Reward:  -1051.5665445767015\n",
            "Current TD Error:  14.041106251539022\n",
            "\n",
            "\n",
            "Episode:  69\n",
            "Current Train Reward:  -1104.498793065496\n",
            "Current TD Error:  11.477383167502671\n",
            "\n",
            "\n",
            "Episode:  70\n",
            "Current Train Reward:  -1105.6790645575138\n",
            "Current TD Error:  10.557339756970137\n",
            "\n",
            "\n",
            "Episode:  71\n",
            "Current Train Reward:  -1097.6411774953833\n",
            "Current TD Error:  12.490279107413158\n",
            "\n",
            "\n",
            "Episode:  72\n",
            "Current Train Reward:  -1055.6820070781346\n",
            "Current TD Error:  13.871677304829944\n",
            "\n",
            "\n",
            "Episode:  73\n",
            "Current Train Reward:  -1058.4733992243005\n",
            "Current TD Error:  14.657008854158612\n",
            "\n",
            "\n",
            "Episode:  74\n",
            "Current Train Reward:  -1074.4834562912697\n",
            "Current TD Error:  13.363026364496688\n",
            "\n",
            "\n",
            "Episode:  75\n",
            "Current Train Reward:  -1085.4281006034323\n",
            "Current TD Error:  13.00435902838034\n",
            "\n",
            "\n",
            "Episode:  76\n",
            "Current Train Reward:  -1061.640828572538\n",
            "Current TD Error:  15.869700770834005\n",
            "\n",
            "\n",
            "Episode:  77\n",
            "Current Train Reward:  -1091.150166877162\n",
            "Current TD Error:  13.476202180423678\n",
            "\n",
            "\n",
            "Episode:  78\n",
            "Current Train Reward:  -1060.4821761759126\n",
            "Current TD Error:  14.238357164287953\n",
            "\n",
            "\n",
            "Episode:  79\n",
            "Current Train Reward:  -1053.6800682516255\n",
            "Current TD Error:  12.858575219619036\n",
            "\n",
            "\n",
            "Episode:  80\n",
            "Current Train Reward:  -1091.1461925699932\n",
            "Current TD Error:  11.887794714621306\n",
            "\n",
            "\n",
            "Episode:  81\n",
            "Current Train Reward:  -1092.3348206094809\n",
            "Current TD Error:  11.583924628056204\n",
            "\n",
            "\n",
            "Episode:  82\n",
            "Current Train Reward:  -1084.3263349715846\n",
            "Current TD Error:  12.233128881290915\n",
            "\n",
            "\n",
            "Episode:  83\n",
            "Current Train Reward:  -1099.7293507627087\n",
            "Current TD Error:  10.228289142826739\n",
            "\n",
            "\n",
            "Episode:  84\n",
            "Current Train Reward:  -1064.3889172877462\n",
            "Current TD Error:  12.873500629790339\n",
            "\n",
            "\n",
            "Episode:  85\n",
            "Current Train Reward:  -1074.7972710322406\n",
            "Current TD Error:  13.79763507945643\n",
            "\n",
            "\n",
            "Episode:  86\n",
            "Current Train Reward:  -1085.545948145226\n",
            "Current TD Error:  12.199276508400212\n",
            "\n",
            "\n",
            "Episode:  87\n",
            "Current Train Reward:  -1058.3198740038897\n",
            "Current TD Error:  14.008142863076497\n",
            "\n",
            "\n",
            "Episode:  88\n",
            "Current Train Reward:  -1087.5317061626383\n",
            "Current TD Error:  12.854560609100089\n",
            "\n",
            "\n",
            "Episode:  89\n",
            "Current Train Reward:  -1095.2778834325654\n",
            "Current TD Error:  10.852983250405\n",
            "\n",
            "\n",
            "Episode:  90\n",
            "Current Train Reward:  -1084.9626335902672\n",
            "Current TD Error:  11.529974754529237\n",
            "\n",
            "\n",
            "Episode:  91\n",
            "Current Train Reward:  -1058.797025236577\n",
            "Current TD Error:  15.545192675381745\n",
            "\n",
            "\n",
            "Episode:  92\n",
            "Current Train Reward:  -1096.2093052418697\n",
            "Current TD Error:  11.102670989988825\n",
            "\n",
            "\n",
            "Episode:  93\n",
            "Current Train Reward:  -1060.6976405517644\n",
            "Current TD Error:  14.809415288539807\n",
            "\n",
            "\n",
            "Episode:  94\n",
            "Current Train Reward:  -1061.7475201325233\n",
            "Current TD Error:  15.001387668497395\n",
            "\n",
            "\n",
            "Episode:  95\n",
            "Current Train Reward:  -1074.3822810567995\n",
            "Current TD Error:  13.9439543240893\n",
            "\n",
            "\n",
            "Episode:  96\n",
            "Current Train Reward:  -1110.5811061339566\n",
            "Current TD Error:  15.833924648640577\n",
            "\n",
            "\n",
            "Episode:  97\n",
            "Current Train Reward:  -1084.4608413724368\n",
            "Current TD Error:  12.44325831493242\n",
            "\n",
            "\n",
            "Episode:  98\n",
            "Current Train Reward:  -1061.5458178597562\n",
            "Current TD Error:  13.161400415644238\n",
            "\n",
            "\n",
            "Episode:  99\n",
            "Current Train Reward:  -1072.2060910754599\n",
            "Current TD Error:  13.02655158075615\n",
            "\n",
            "\n",
            "Episode:  100\n",
            "Current Train Reward:  -1084.1502693905404\n",
            "Current TD Error:  12.426868838797926\n",
            "\n",
            "\n",
            "Current Test Reward:  -1144.5578187895153\n",
            "Episode:  101\n",
            "Current Train Reward:  -1066.133086262038\n",
            "Current TD Error:  14.725837648354332\n",
            "\n",
            "\n",
            "Episode:  102\n",
            "Current Train Reward:  -1091.5777236244817\n",
            "Current TD Error:  11.523301730946026\n",
            "\n",
            "\n",
            "Episode:  103\n",
            "Current Train Reward:  -1069.8753330072727\n",
            "Current TD Error:  13.092092403279228\n",
            "\n",
            "\n",
            "Episode:  104\n",
            "Current Train Reward:  -1079.1390104593997\n",
            "Current TD Error:  13.287909556937956\n",
            "\n",
            "\n",
            "Episode:  105\n",
            "Current Train Reward:  -1079.9719433453934\n",
            "Current TD Error:  11.500536213243098\n",
            "\n",
            "\n",
            "Episode:  106\n",
            "Current Train Reward:  -1049.915808892055\n",
            "Current TD Error:  14.014806586904436\n",
            "\n",
            "\n",
            "Episode:  107\n",
            "Current Train Reward:  -1098.7684307900258\n",
            "Current TD Error:  11.94373975252128\n",
            "\n",
            "\n",
            "Episode:  108\n",
            "Current Train Reward:  -1091.1120015645315\n",
            "Current TD Error:  10.72621795037785\n",
            "\n",
            "\n",
            "Episode:  109\n",
            "Current Train Reward:  -1090.5647792301613\n",
            "Current TD Error:  11.449171660327075\n",
            "\n",
            "\n",
            "Episode:  110\n",
            "Current Train Reward:  -1079.811834830104\n",
            "Current TD Error:  11.453258017226712\n",
            "\n",
            "\n",
            "Episode:  111\n",
            "Current Train Reward:  -1070.4447660291123\n",
            "Current TD Error:  14.219553797541272\n",
            "\n",
            "\n",
            "Episode:  112\n",
            "Current Train Reward:  -1101.0812919032437\n",
            "Current TD Error:  12.646062206615357\n",
            "\n",
            "\n",
            "Episode:  113\n",
            "Current Train Reward:  -1071.6582946019523\n",
            "Current TD Error:  11.348610751109204\n",
            "\n",
            "\n",
            "Episode:  114\n",
            "Current Train Reward:  -1068.138910596811\n",
            "Current TD Error:  11.889953914941355\n",
            "\n",
            "\n",
            "Episode:  115\n",
            "Current Train Reward:  -1057.4232865261881\n",
            "Current TD Error:  15.098306370595946\n",
            "\n",
            "\n",
            "Episode:  116\n",
            "Current Train Reward:  -1068.8595220608881\n",
            "Current TD Error:  12.862839058291133\n",
            "\n",
            "\n",
            "Episode:  117\n",
            "Current Train Reward:  -1062.0995149544824\n",
            "Current TD Error:  12.90440076171081\n",
            "\n",
            "\n",
            "Episode:  118\n",
            "Current Train Reward:  -1085.347855192526\n",
            "Current TD Error:  11.63456362654691\n",
            "\n",
            "\n",
            "Episode:  119\n",
            "Current Train Reward:  -1059.861156648294\n",
            "Current TD Error:  13.38927496093398\n",
            "\n",
            "\n",
            "Episode:  120\n",
            "Current Train Reward:  -1088.0714388442211\n",
            "Current TD Error:  11.858195633533477\n",
            "\n",
            "\n",
            "Episode:  121\n",
            "Current Train Reward:  -1068.299778835899\n",
            "Current TD Error:  12.329442185994937\n",
            "\n",
            "\n",
            "Episode:  122\n",
            "Current Train Reward:  -1082.3686670201748\n",
            "Current TD Error:  12.380854236252613\n",
            "\n",
            "\n",
            "Episode:  123\n",
            "Current Train Reward:  -1088.088052905059\n",
            "Current TD Error:  11.306783406970535\n",
            "\n",
            "\n",
            "Episode:  124\n",
            "Current Train Reward:  -1048.7374324600132\n",
            "Current TD Error:  16.435160048422823\n",
            "\n",
            "\n",
            "Episode:  125\n",
            "Current Train Reward:  -1072.198296247418\n",
            "Current TD Error:  12.385714355774898\n",
            "\n",
            "\n",
            "Episode:  126\n",
            "Current Train Reward:  -1051.2057733027245\n",
            "Current TD Error:  15.616294356199418\n",
            "\n",
            "\n",
            "Episode:  127\n",
            "Current Train Reward:  -1089.78405498647\n",
            "Current TD Error:  11.532256130425498\n",
            "\n",
            "\n",
            "Episode:  128\n",
            "Current Train Reward:  -1048.0197176477989\n",
            "Current TD Error:  12.361196906957007\n",
            "\n",
            "\n",
            "Episode:  129\n",
            "Current Train Reward:  -1063.1735019312437\n",
            "Current TD Error:  11.55278299670659\n",
            "\n",
            "\n",
            "Episode:  130\n",
            "Current Train Reward:  -1079.4612127620337\n",
            "Current TD Error:  12.030376414559878\n",
            "\n",
            "\n",
            "Episode:  131\n",
            "Current Train Reward:  -1062.4384996391716\n",
            "Current TD Error:  13.520219637617462\n",
            "\n",
            "\n",
            "Episode:  132\n",
            "Current Train Reward:  -1071.6085779480775\n",
            "Current TD Error:  11.388041422970401\n",
            "\n",
            "\n",
            "Episode:  133\n",
            "Current Train Reward:  -1072.0678781744523\n",
            "Current TD Error:  12.311146117671743\n",
            "\n",
            "\n",
            "Episode:  134\n",
            "Current Train Reward:  -1092.7823767021173\n",
            "Current TD Error:  10.46091099340211\n",
            "\n",
            "\n",
            "Episode:  135\n",
            "Current Train Reward:  -1076.584420985829\n",
            "Current TD Error:  12.071869295940042\n",
            "\n",
            "\n",
            "Episode:  136\n",
            "Current Train Reward:  -1065.2715381530477\n",
            "Current TD Error:  16.188938591797875\n",
            "\n",
            "\n",
            "Episode:  137\n",
            "Current Train Reward:  -1057.1712323326726\n",
            "Current TD Error:  13.849157348860262\n",
            "\n",
            "\n",
            "Episode:  138\n",
            "Current Train Reward:  -1050.0432357787706\n",
            "Current TD Error:  13.22329474515644\n",
            "\n",
            "\n",
            "Episode:  139\n",
            "Current Train Reward:  -1071.79594121836\n",
            "Current TD Error:  12.669980715851716\n",
            "\n",
            "\n",
            "Episode:  140\n",
            "Current Train Reward:  -1094.5134394334978\n",
            "Current TD Error:  11.811763667655102\n",
            "\n",
            "\n",
            "Episode:  141\n",
            "Current Train Reward:  -1096.2925440613099\n",
            "Current TD Error:  13.228777153814004\n",
            "\n",
            "\n",
            "Episode:  142\n",
            "Current Train Reward:  -1089.6676694591358\n",
            "Current TD Error:  10.91735134838827\n",
            "\n",
            "\n",
            "Episode:  143\n",
            "Current Train Reward:  -1075.712517815327\n",
            "Current TD Error:  13.463104927680005\n",
            "\n",
            "\n",
            "Episode:  144\n",
            "Current Train Reward:  -1045.2974497757634\n",
            "Current TD Error:  14.128105477998457\n",
            "\n",
            "\n",
            "Episode:  145\n",
            "Current Train Reward:  -1048.7332980770634\n",
            "Current TD Error:  13.108863174109151\n",
            "\n",
            "\n",
            "Episode:  146\n",
            "Current Train Reward:  -1088.6468303639338\n",
            "Current TD Error:  11.42946965322882\n",
            "\n",
            "\n",
            "Episode:  147\n",
            "Current Train Reward:  -1056.712186054327\n",
            "Current TD Error:  12.37041235443562\n",
            "\n",
            "\n",
            "Episode:  148\n",
            "Current Train Reward:  -1049.5796577540154\n",
            "Current TD Error:  12.362999626757318\n",
            "\n",
            "\n",
            "Episode:  149\n",
            "Current Train Reward:  -1061.2752324755168\n",
            "Current TD Error:  13.52246856234678\n",
            "\n",
            "\n",
            "Episode:  150\n",
            "Current Train Reward:  -1084.8311970635193\n",
            "Current TD Error:  11.830038611683099\n",
            "\n",
            "\n",
            "Episode:  151\n",
            "Current Train Reward:  -1049.6198989360864\n",
            "Current TD Error:  15.760529613891368\n",
            "\n",
            "\n",
            "Episode:  152\n",
            "Current Train Reward:  -1056.980563869068\n",
            "Current TD Error:  12.345034186576843\n",
            "\n",
            "\n",
            "Episode:  153\n",
            "Current Train Reward:  -1063.8291148165472\n",
            "Current TD Error:  13.795593827835438\n",
            "\n",
            "\n",
            "Episode:  154\n",
            "Current Train Reward:  -1062.6032747305976\n",
            "Current TD Error:  15.284515813524445\n",
            "\n",
            "\n",
            "Episode:  155\n",
            "Current Train Reward:  -1076.847671193238\n",
            "Current TD Error:  12.837505051880843\n",
            "\n",
            "\n",
            "Episode:  156\n",
            "Current Train Reward:  -1066.5952625986868\n",
            "Current TD Error:  13.222915926614759\n",
            "\n",
            "\n",
            "Episode:  157\n",
            "Current Train Reward:  -1047.4143183722451\n",
            "Current TD Error:  14.3831899963632\n",
            "\n",
            "\n",
            "Episode:  158\n",
            "Current Train Reward:  -1086.8891902395064\n",
            "Current TD Error:  12.387761569494083\n",
            "\n",
            "\n",
            "Episode:  159\n",
            "Current Train Reward:  -1058.6562351987277\n",
            "Current TD Error:  12.512950967847171\n",
            "\n",
            "\n",
            "Episode:  160\n",
            "Current Train Reward:  -1092.764209171207\n",
            "Current TD Error:  10.559736180209166\n",
            "\n",
            "\n",
            "Episode:  161\n",
            "Current Train Reward:  -1075.4355825267733\n",
            "Current TD Error:  11.172887667033338\n",
            "\n",
            "\n",
            "Episode:  162\n",
            "Current Train Reward:  -1053.789000723033\n",
            "Current TD Error:  13.787191111710925\n",
            "\n",
            "\n",
            "Episode:  163\n",
            "Current Train Reward:  -1072.780790800186\n",
            "Current TD Error:  11.893595685050219\n",
            "\n",
            "\n",
            "Episode:  164\n",
            "Current Train Reward:  -1044.2292371805252\n",
            "Current TD Error:  12.479161883057403\n",
            "\n",
            "\n",
            "Episode:  165\n",
            "Current Train Reward:  -1052.0622949060344\n",
            "Current TD Error:  14.28556656733996\n",
            "\n",
            "\n",
            "Episode:  166\n",
            "Current Train Reward:  -1060.0615978323654\n",
            "Current TD Error:  12.527931280717283\n",
            "\n",
            "\n",
            "Episode:  167\n",
            "Current Train Reward:  -1064.8027469409587\n",
            "Current TD Error:  10.96871039892355\n",
            "\n",
            "\n",
            "Episode:  168\n",
            "Current Train Reward:  -1068.4014858514795\n",
            "Current TD Error:  11.523605113740913\n",
            "\n",
            "\n",
            "Episode:  169\n",
            "Current Train Reward:  -1086.0967271059476\n",
            "Current TD Error:  11.022760782866444\n",
            "\n",
            "\n",
            "Episode:  170\n",
            "Current Train Reward:  -1063.9178876790563\n",
            "Current TD Error:  12.854930873241956\n",
            "\n",
            "\n",
            "Episode:  171\n",
            "Current Train Reward:  -1072.0068397793434\n",
            "Current TD Error:  11.768887909234557\n",
            "\n",
            "\n",
            "Episode:  172\n",
            "Current Train Reward:  -1088.459009013192\n",
            "Current TD Error:  9.782619582314828\n",
            "\n",
            "\n",
            "Episode:  173\n",
            "Current Train Reward:  -1084.0975964837323\n",
            "Current TD Error:  10.994454013694568\n",
            "\n",
            "\n",
            "Episode:  174\n",
            "Current Train Reward:  -1054.1241837416796\n",
            "Current TD Error:  12.787930890544496\n",
            "\n",
            "\n",
            "Episode:  175\n",
            "Current Train Reward:  -1079.2262982294656\n",
            "Current TD Error:  12.279126522701597\n",
            "\n",
            "\n",
            "Episode:  176\n",
            "Current Train Reward:  -1054.1058902839147\n",
            "Current TD Error:  13.22515792819255\n",
            "\n",
            "\n",
            "Episode:  177\n",
            "Current Train Reward:  -1087.5083990521434\n",
            "Current TD Error:  13.840563890070488\n",
            "\n",
            "\n",
            "Episode:  178\n",
            "Current Train Reward:  -1064.2987700701474\n",
            "Current TD Error:  12.373254047500422\n",
            "\n",
            "\n",
            "Episode:  179\n",
            "Current Train Reward:  -1079.9946358936677\n",
            "Current TD Error:  12.95038408364976\n",
            "\n",
            "\n",
            "Episode:  180\n",
            "Current Train Reward:  -1047.9900641782153\n",
            "Current TD Error:  16.545810151004073\n",
            "\n",
            "\n",
            "Episode:  181\n",
            "Current Train Reward:  -1052.3509088204596\n",
            "Current TD Error:  20.413078624284008\n",
            "\n",
            "\n",
            "Episode:  182\n",
            "Current Train Reward:  -1038.171754078603\n",
            "Current TD Error:  12.554245706588311\n",
            "\n",
            "\n",
            "Episode:  183\n",
            "Current Train Reward:  -1073.905682152773\n",
            "Current TD Error:  10.613081645344481\n",
            "\n",
            "\n",
            "Episode:  184\n",
            "Current Train Reward:  -1077.5976164115023\n",
            "Current TD Error:  12.050428607375688\n",
            "\n",
            "\n",
            "Episode:  185\n",
            "Current Train Reward:  -1054.3742865803226\n",
            "Current TD Error:  11.491115723205894\n",
            "\n",
            "\n",
            "Episode:  186\n",
            "Current Train Reward:  -1068.7671928791963\n",
            "Current TD Error:  13.836036772947182\n",
            "\n",
            "\n",
            "Episode:  187\n",
            "Current Train Reward:  -1083.1999888366781\n",
            "Current TD Error:  11.867896886606438\n",
            "\n",
            "\n",
            "Episode:  188\n",
            "Current Train Reward:  -1062.4213995566681\n",
            "Current TD Error:  10.544088586702077\n",
            "\n",
            "\n",
            "Episode:  189\n",
            "Current Train Reward:  -1077.95048533548\n",
            "Current TD Error:  13.088123262904826\n",
            "\n",
            "\n",
            "Episode:  190\n",
            "Current Train Reward:  -1040.9430246722109\n",
            "Current TD Error:  13.887382954336955\n",
            "\n",
            "\n",
            "Episode:  191\n",
            "Current Train Reward:  -1070.1422684671788\n",
            "Current TD Error:  10.819493700584577\n",
            "\n",
            "\n",
            "Episode:  192\n",
            "Current Train Reward:  -1082.6908951664548\n",
            "Current TD Error:  13.065827006882506\n",
            "\n",
            "\n",
            "Episode:  193\n",
            "Current Train Reward:  -1073.980456150621\n",
            "Current TD Error:  11.762618677991004\n",
            "\n",
            "\n",
            "Episode:  194\n",
            "Current Train Reward:  -1078.1047187724917\n",
            "Current TD Error:  10.752636015808514\n",
            "\n",
            "\n",
            "Episode:  195\n",
            "Current Train Reward:  -1044.2147960409402\n",
            "Current TD Error:  15.540851699054206\n",
            "\n",
            "\n",
            "Episode:  196\n",
            "Current Train Reward:  -1080.0764055264879\n",
            "Current TD Error:  10.617596688971561\n",
            "\n",
            "\n",
            "Episode:  197\n",
            "Current Train Reward:  -1055.1135787653109\n",
            "Current TD Error:  11.867946133710053\n",
            "\n",
            "\n",
            "Episode:  198\n",
            "Current Train Reward:  -1035.1324242012522\n",
            "Current TD Error:  14.030070356485554\n",
            "\n",
            "\n",
            "Episode:  199\n",
            "Current Train Reward:  -1048.1637888228495\n",
            "Current TD Error:  12.751692799638086\n",
            "\n",
            "\n",
            "Episode:  200\n",
            "Current Train Reward:  -1079.8683688348472\n",
            "Current TD Error:  11.482177307249426\n",
            "\n",
            "\n",
            "Current Test Reward:  -1069.4402583774736\n",
            "Episode:  201\n",
            "Current Train Reward:  -1052.2422678077899\n",
            "Current TD Error:  14.325866423356269\n",
            "\n",
            "\n",
            "Episode:  202\n",
            "Current Train Reward:  -1048.9213800098844\n",
            "Current TD Error:  14.376124654493617\n",
            "\n",
            "\n",
            "Episode:  203\n",
            "Current Train Reward:  -1045.0668485369692\n",
            "Current TD Error:  11.834189476291902\n",
            "\n",
            "\n",
            "Episode:  204\n",
            "Current Train Reward:  -1069.3126668943305\n",
            "Current TD Error:  10.358865141139987\n",
            "\n",
            "\n",
            "Episode:  205\n",
            "Current Train Reward:  -1078.6381280374112\n",
            "Current TD Error:  14.100153707336867\n",
            "\n",
            "\n",
            "Episode:  206\n",
            "Current Train Reward:  -1062.030103299766\n",
            "Current TD Error:  11.98568574345161\n",
            "\n",
            "\n",
            "Episode:  207\n",
            "Current Train Reward:  -1073.335072684684\n",
            "Current TD Error:  13.081405741647249\n",
            "\n",
            "\n",
            "Episode:  208\n",
            "Current Train Reward:  -1086.1933469988332\n",
            "Current TD Error:  8.694791176988618\n",
            "\n",
            "\n",
            "Episode:  209\n",
            "Current Train Reward:  -1063.8739312076225\n",
            "Current TD Error:  11.184034247008446\n",
            "\n",
            "\n",
            "Episode:  210\n",
            "Current Train Reward:  -1043.4649807056617\n",
            "Current TD Error:  14.487193037967847\n",
            "\n",
            "\n",
            "Episode:  211\n",
            "Current Train Reward:  -1085.8405845130533\n",
            "Current TD Error:  16.00192315017715\n",
            "\n",
            "\n",
            "Episode:  212\n",
            "Current Train Reward:  -1072.1558913529086\n",
            "Current TD Error:  11.20565828170567\n",
            "\n",
            "\n",
            "Episode:  213\n",
            "Current Train Reward:  -1081.2530235404272\n",
            "Current TD Error:  9.002975711325108\n",
            "\n",
            "\n",
            "Episode:  214\n",
            "Current Train Reward:  -1074.9145075224787\n",
            "Current TD Error:  9.655853025749375\n",
            "\n",
            "\n",
            "Episode:  215\n",
            "Current Train Reward:  -1057.6577930088445\n",
            "Current TD Error:  12.449131912248568\n",
            "\n",
            "\n",
            "Episode:  216\n",
            "Current Train Reward:  -1076.3372889302468\n",
            "Current TD Error:  16.562330870574414\n",
            "\n",
            "\n",
            "Episode:  217\n",
            "Current Train Reward:  -1038.0809180273625\n",
            "Current TD Error:  12.823757690025248\n",
            "\n",
            "\n",
            "Episode:  218\n",
            "Current Train Reward:  -1063.9636966390585\n",
            "Current TD Error:  9.948734773570356\n",
            "\n",
            "\n",
            "Episode:  219\n",
            "Current Train Reward:  -1038.1117911705371\n",
            "Current TD Error:  11.345940848845046\n",
            "\n",
            "\n",
            "Episode:  220\n",
            "Current Train Reward:  -1058.9118121330807\n",
            "Current TD Error:  9.686299611917645\n",
            "\n",
            "\n",
            "Episode:  221\n",
            "Current Train Reward:  -1039.7000543191664\n",
            "Current TD Error:  15.372351583902743\n",
            "\n",
            "\n",
            "Episode:  222\n",
            "Current Train Reward:  -1066.556067186267\n",
            "Current TD Error:  9.933193185342047\n",
            "\n",
            "\n",
            "Episode:  223\n",
            "Current Train Reward:  -1059.4770022479172\n",
            "Current TD Error:  12.468660083629302\n",
            "\n",
            "\n",
            "Episode:  224\n",
            "Current Train Reward:  -1059.775177623921\n",
            "Current TD Error:  12.703839372589405\n",
            "\n",
            "\n",
            "Episode:  225\n",
            "Current Train Reward:  -1089.4440980988054\n",
            "Current TD Error:  10.24333114231842\n",
            "\n",
            "\n",
            "Episode:  226\n",
            "Current Train Reward:  -1034.4601160088198\n",
            "Current TD Error:  12.163913442515202\n",
            "\n",
            "\n",
            "Episode:  227\n",
            "Current Train Reward:  -1070.6397212096433\n",
            "Current TD Error:  10.755915211217626\n",
            "\n",
            "\n",
            "Episode:  228\n",
            "Current Train Reward:  -1067.1786748611385\n",
            "Current TD Error:  8.338962996926604\n",
            "\n",
            "\n",
            "Episode:  229\n",
            "Current Train Reward:  -1072.8795419694907\n",
            "Current TD Error:  10.256887965651345\n",
            "\n",
            "\n",
            "Episode:  230\n",
            "Current Train Reward:  -1055.3585899435654\n",
            "Current TD Error:  10.33473594900792\n",
            "\n",
            "\n",
            "Episode:  231\n",
            "Current Train Reward:  -1087.9524397331961\n",
            "Current TD Error:  11.71336191790401\n",
            "\n",
            "\n",
            "Episode:  232\n",
            "Current Train Reward:  -1060.5088620719182\n",
            "Current TD Error:  11.08949065449843\n",
            "\n",
            "\n",
            "Episode:  233\n",
            "Current Train Reward:  -1067.331785500448\n",
            "Current TD Error:  10.58240356199119\n",
            "\n",
            "\n",
            "Episode:  234\n",
            "Current Train Reward:  -1041.5765380612188\n",
            "Current TD Error:  12.203744296201359\n",
            "\n",
            "\n",
            "Episode:  235\n",
            "Current Train Reward:  -1051.4402349365469\n",
            "Current TD Error:  12.003475194285048\n",
            "\n",
            "\n",
            "Episode:  236\n",
            "Current Train Reward:  -1079.3682670939204\n",
            "Current TD Error:  13.21582824787664\n",
            "\n",
            "\n",
            "Episode:  237\n",
            "Current Train Reward:  -1064.271658787177\n",
            "Current TD Error:  12.615935602027552\n",
            "\n",
            "\n",
            "Episode:  238\n",
            "Current Train Reward:  -1073.0153629790047\n",
            "Current TD Error:  10.664119905765347\n",
            "\n",
            "\n",
            "Episode:  239\n",
            "Current Train Reward:  -1043.1597719639917\n",
            "Current TD Error:  11.187936361898307\n",
            "\n",
            "\n",
            "Episode:  240\n",
            "Current Train Reward:  -1070.086785493988\n",
            "Current TD Error:  8.510330620514688\n",
            "\n",
            "\n",
            "Episode:  241\n",
            "Current Train Reward:  -1080.2519036333629\n",
            "Current TD Error:  10.582246749772363\n",
            "\n",
            "\n",
            "Episode:  242\n",
            "Current Train Reward:  -1070.1046625209285\n",
            "Current TD Error:  10.497216225035656\n",
            "\n",
            "\n",
            "Episode:  243\n",
            "Current Train Reward:  -1040.0331633496355\n",
            "Current TD Error:  13.291844117923192\n",
            "\n",
            "\n",
            "Episode:  244\n",
            "Current Train Reward:  -1075.938370334581\n",
            "Current TD Error:  11.475565908962972\n",
            "\n",
            "\n",
            "Episode:  245\n",
            "Current Train Reward:  -1080.2197721977884\n",
            "Current TD Error:  8.865825370270938\n",
            "\n",
            "\n",
            "Episode:  246\n",
            "Current Train Reward:  -1081.4737979901458\n",
            "Current TD Error:  11.291134383169359\n",
            "\n",
            "\n",
            "Episode:  247\n",
            "Current Train Reward:  -1042.6291633706164\n",
            "Current TD Error:  12.055950113411697\n",
            "\n",
            "\n",
            "Episode:  248\n",
            "Current Train Reward:  -1048.5359165228533\n",
            "Current TD Error:  11.057632078956729\n",
            "\n",
            "\n",
            "Episode:  249\n",
            "Current Train Reward:  -1063.5984354696352\n",
            "Current TD Error:  9.340004722646222\n",
            "\n",
            "\n",
            "Episode:  250\n",
            "Current Train Reward:  -1074.48408921264\n",
            "Current TD Error:  11.086338917979328\n",
            "\n",
            "\n",
            "Episode:  251\n",
            "Current Train Reward:  -1073.141093594063\n",
            "Current TD Error:  10.70525373883009\n",
            "\n",
            "\n",
            "Episode:  252\n",
            "Current Train Reward:  -1080.6503076785464\n",
            "Current TD Error:  9.14928243546622\n",
            "\n",
            "\n",
            "Episode:  253\n",
            "Current Train Reward:  -1069.2937672024448\n",
            "Current TD Error:  9.487180663616373\n",
            "\n",
            "\n",
            "Episode:  254\n",
            "Current Train Reward:  -1045.2020629414342\n",
            "Current TD Error:  10.554263197789737\n",
            "\n",
            "\n",
            "Episode:  255\n",
            "Current Train Reward:  -1067.0400942217661\n",
            "Current TD Error:  10.393924982201034\n",
            "\n",
            "\n",
            "Episode:  256\n",
            "Current Train Reward:  -1049.1817970660018\n",
            "Current TD Error:  11.699220589542062\n",
            "\n",
            "\n",
            "Episode:  257\n",
            "Current Train Reward:  -1052.7877649861252\n",
            "Current TD Error:  11.428262760832247\n",
            "\n",
            "\n",
            "Episode:  258\n",
            "Current Train Reward:  -1071.0838934177239\n",
            "Current TD Error:  8.812370976397078\n",
            "\n",
            "\n",
            "Episode:  259\n",
            "Current Train Reward:  -1066.1352411704481\n",
            "Current TD Error:  9.921385019319782\n",
            "\n",
            "\n",
            "Episode:  260\n",
            "Current Train Reward:  -1056.6054132294737\n",
            "Current TD Error:  10.848803756117464\n",
            "\n",
            "\n",
            "Episode:  261\n",
            "Current Train Reward:  -1036.215559025513\n",
            "Current TD Error:  14.470598668685836\n",
            "\n",
            "\n",
            "Episode:  262\n",
            "Current Train Reward:  -1053.5383808418105\n",
            "Current TD Error:  11.783135828466271\n",
            "\n",
            "\n",
            "Episode:  263\n",
            "Current Train Reward:  -1037.6869712329253\n",
            "Current TD Error:  12.213039627453524\n",
            "\n",
            "\n",
            "Episode:  264\n",
            "Current Train Reward:  -1043.5965934036951\n",
            "Current TD Error:  12.580444680957946\n",
            "\n",
            "\n",
            "Episode:  265\n",
            "Current Train Reward:  -1034.4846873330048\n",
            "Current TD Error:  13.185658385466528\n",
            "\n",
            "\n",
            "Episode:  266\n",
            "Current Train Reward:  -1039.0305347837573\n",
            "Current TD Error:  13.653723048096731\n",
            "\n",
            "\n",
            "Episode:  267\n",
            "Current Train Reward:  -1053.8326248972987\n",
            "Current TD Error:  9.91347758339276\n",
            "\n",
            "\n",
            "Episode:  268\n",
            "Current Train Reward:  -1077.1201737534195\n",
            "Current TD Error:  10.787286079527691\n",
            "\n",
            "\n",
            "Episode:  269\n",
            "Current Train Reward:  -1042.7282977059926\n",
            "Current TD Error:  14.933738088960407\n",
            "\n",
            "\n",
            "Episode:  270\n",
            "Current Train Reward:  -1082.2893972585216\n",
            "Current TD Error:  10.136240870227203\n",
            "\n",
            "\n",
            "Episode:  271\n",
            "Current Train Reward:  -1059.4601290572787\n",
            "Current TD Error:  11.37031366288904\n",
            "\n",
            "\n",
            "Episode:  272\n",
            "Current Train Reward:  -1076.8790931054468\n",
            "Current TD Error:  12.596340491818834\n",
            "\n",
            "\n",
            "Episode:  273\n",
            "Current Train Reward:  -1081.8684091258697\n",
            "Current TD Error:  11.830607045738748\n",
            "\n",
            "\n",
            "Episode:  274\n",
            "Current Train Reward:  -1057.26238718099\n",
            "Current TD Error:  10.511234352743568\n",
            "\n",
            "\n",
            "Episode:  275\n",
            "Current Train Reward:  -1079.9964325170226\n",
            "Current TD Error:  11.40053481886213\n",
            "\n",
            "\n",
            "Episode:  276\n",
            "Current Train Reward:  -1057.1483520457202\n",
            "Current TD Error:  11.803758125521487\n",
            "\n",
            "\n",
            "Episode:  277\n",
            "Current Train Reward:  -1077.421662559755\n",
            "Current TD Error:  10.296796752196002\n",
            "\n",
            "\n",
            "Episode:  278\n",
            "Current Train Reward:  -1063.4099493100962\n",
            "Current TD Error:  10.399065670822257\n",
            "\n",
            "\n",
            "Episode:  279\n",
            "Current Train Reward:  -1073.7580652889196\n",
            "Current TD Error:  9.799402203198635\n",
            "\n",
            "\n",
            "Episode:  280\n",
            "Current Train Reward:  -1055.1009996201399\n",
            "Current TD Error:  10.299364927340495\n",
            "\n",
            "\n",
            "Episode:  281\n",
            "Current Train Reward:  -1081.6624280095457\n",
            "Current TD Error:  10.673169211241953\n",
            "\n",
            "\n",
            "Episode:  282\n",
            "Current Train Reward:  -1080.400595104433\n",
            "Current TD Error:  9.787990380067647\n",
            "\n",
            "\n",
            "Episode:  283\n",
            "Current Train Reward:  -1049.766928956765\n",
            "Current TD Error:  11.198703936752784\n",
            "\n",
            "\n",
            "Episode:  284\n",
            "Current Train Reward:  -1039.029339206137\n",
            "Current TD Error:  12.429900451935199\n",
            "\n",
            "\n",
            "Episode:  285\n",
            "Current Train Reward:  -1054.3740564612099\n",
            "Current TD Error:  14.184326469323235\n",
            "\n",
            "\n",
            "Episode:  286\n",
            "Current Train Reward:  -1076.679087638092\n",
            "Current TD Error:  10.332870314314132\n",
            "\n",
            "\n",
            "Episode:  287\n",
            "Current Train Reward:  -1074.2053470910287\n",
            "Current TD Error:  11.519057010973306\n",
            "\n",
            "\n",
            "Episode:  288\n",
            "Current Train Reward:  -1046.380841746697\n",
            "Current TD Error:  10.921389204261583\n",
            "\n",
            "\n",
            "Episode:  289\n",
            "Current Train Reward:  -1079.766730296825\n",
            "Current TD Error:  10.795040234069361\n",
            "\n",
            "\n",
            "Episode:  290\n",
            "Current Train Reward:  -1070.9624992837455\n",
            "Current TD Error:  11.760369920865996\n",
            "\n",
            "\n",
            "Episode:  291\n",
            "Current Train Reward:  -1051.6847089257208\n",
            "Current TD Error:  11.382326399378865\n",
            "\n",
            "\n",
            "Episode:  292\n",
            "Current Train Reward:  -1072.046574033327\n",
            "Current TD Error:  10.6250831237217\n",
            "\n",
            "\n",
            "Episode:  293\n",
            "Current Train Reward:  -1080.1882790806003\n",
            "Current TD Error:  10.548671595465258\n",
            "\n",
            "\n",
            "Episode:  294\n",
            "Current Train Reward:  -1030.516313724394\n",
            "Current TD Error:  10.812495162888677\n",
            "\n",
            "\n",
            "Episode:  295\n",
            "Current Train Reward:  -1055.2992224580007\n",
            "Current TD Error:  16.187520483144233\n",
            "\n",
            "\n",
            "Episode:  296\n",
            "Current Train Reward:  -1048.6149705507896\n",
            "Current TD Error:  13.659734826388643\n",
            "\n",
            "\n",
            "Episode:  297\n",
            "Current Train Reward:  -1078.6765826850396\n",
            "Current TD Error:  8.187671081285732\n",
            "\n",
            "\n",
            "Episode:  298\n",
            "Current Train Reward:  -1072.1039087409686\n",
            "Current TD Error:  10.191694835140778\n",
            "\n",
            "\n",
            "Episode:  299\n",
            "Current Train Reward:  -1057.7608949586233\n",
            "Current TD Error:  10.611472837576306\n",
            "\n",
            "\n",
            "Episode:  300\n",
            "Current Train Reward:  -1030.6719282023766\n",
            "Current TD Error:  10.923136783711069\n",
            "\n",
            "\n",
            "Current Test Reward:  -1147.7891422010835\n",
            "Episode:  301\n",
            "Current Train Reward:  -1058.9729717736857\n",
            "Current TD Error:  10.698626784285223\n",
            "\n",
            "\n",
            "Episode:  302\n",
            "Current Train Reward:  -1050.8264516270556\n",
            "Current TD Error:  10.390622825024204\n",
            "\n",
            "\n",
            "Episode:  303\n",
            "Current Train Reward:  -1035.7024332365672\n",
            "Current TD Error:  11.221822403044504\n",
            "\n",
            "\n",
            "Episode:  304\n",
            "Current Train Reward:  -1068.0735597183502\n",
            "Current TD Error:  9.547041677211755\n",
            "\n",
            "\n",
            "Episode:  305\n",
            "Current Train Reward:  -1048.8905194701458\n",
            "Current TD Error:  11.294424112766459\n",
            "\n",
            "\n",
            "Episode:  306\n",
            "Current Train Reward:  -1071.1010720018314\n",
            "Current TD Error:  10.588927369140475\n",
            "\n",
            "\n",
            "Episode:  307\n",
            "Current Train Reward:  -1037.0770033415208\n",
            "Current TD Error:  11.175635875180713\n",
            "\n",
            "\n",
            "Episode:  308\n",
            "Current Train Reward:  -1071.5588702251882\n",
            "Current TD Error:  9.066293420382342\n",
            "\n",
            "\n",
            "Episode:  309\n",
            "Current Train Reward:  -1041.559333536907\n",
            "Current TD Error:  10.958335296653276\n",
            "\n",
            "\n",
            "Episode:  310\n",
            "Current Train Reward:  -1037.3995890069923\n",
            "Current TD Error:  11.147220734438902\n",
            "\n",
            "\n",
            "Episode:  311\n",
            "Current Train Reward:  -1043.5517298148545\n",
            "Current TD Error:  13.065986435804094\n",
            "\n",
            "\n",
            "Episode:  312\n",
            "Current Train Reward:  -1054.298318430459\n",
            "Current TD Error:  10.232005013004978\n",
            "\n",
            "\n",
            "Episode:  313\n",
            "Current Train Reward:  -1076.301892704373\n",
            "Current TD Error:  8.248212331992104\n",
            "\n",
            "\n",
            "Episode:  314\n",
            "Current Train Reward:  -1071.33545998143\n",
            "Current TD Error:  9.670419186837865\n",
            "\n",
            "\n",
            "Episode:  315\n",
            "Current Train Reward:  -1041.85384747322\n",
            "Current TD Error:  10.835269442395383\n",
            "\n",
            "\n",
            "Episode:  316\n",
            "Current Train Reward:  -1041.898749789631\n",
            "Current TD Error:  11.323346532796428\n",
            "\n",
            "\n",
            "Episode:  317\n",
            "Current Train Reward:  -1052.7680515808606\n",
            "Current TD Error:  9.817500401155842\n",
            "\n",
            "\n",
            "Episode:  318\n",
            "Current Train Reward:  -1048.1097120578145\n",
            "Current TD Error:  8.608656273761843\n",
            "\n",
            "\n",
            "Episode:  319\n",
            "Current Train Reward:  -1024.760297701441\n",
            "Current TD Error:  10.646549883712918\n",
            "\n",
            "\n",
            "Episode:  320\n",
            "Current Train Reward:  -1073.0077466817102\n",
            "Current TD Error:  11.065331777220178\n",
            "\n",
            "\n",
            "Episode:  321\n",
            "Current Train Reward:  -1072.089548391286\n",
            "Current TD Error:  9.212281976133156\n",
            "\n",
            "\n",
            "Episode:  322\n",
            "Current Train Reward:  -1046.5181059134459\n",
            "Current TD Error:  8.543604306323454\n",
            "\n",
            "\n",
            "Episode:  323\n",
            "Current Train Reward:  -1061.9269545526206\n",
            "Current TD Error:  8.670269733425728\n",
            "\n",
            "\n",
            "Episode:  324\n",
            "Current Train Reward:  -1050.782055880216\n",
            "Current TD Error:  9.278236178378668\n",
            "\n",
            "\n",
            "Episode:  325\n",
            "Current Train Reward:  -1054.5727321540658\n",
            "Current TD Error:  9.500093783955228\n",
            "\n",
            "\n",
            "Episode:  326\n",
            "Current Train Reward:  -1034.5782921330494\n",
            "Current TD Error:  12.049295783149837\n",
            "\n",
            "\n",
            "Episode:  327\n",
            "Current Train Reward:  -1041.0865999565376\n",
            "Current TD Error:  11.946973634431526\n",
            "\n",
            "\n",
            "Episode:  328\n",
            "Current Train Reward:  -1040.207901307056\n",
            "Current TD Error:  9.994479173878283\n",
            "\n",
            "\n",
            "Episode:  329\n",
            "Current Train Reward:  -1070.1458339907254\n",
            "Current TD Error:  11.069571558976213\n",
            "\n",
            "\n",
            "Episode:  330\n",
            "Current Train Reward:  -1050.6557862275347\n",
            "Current TD Error:  9.627309420261343\n",
            "\n",
            "\n",
            "Episode:  331\n",
            "Current Train Reward:  -1044.2984431981738\n",
            "Current TD Error:  10.825426993828284\n",
            "\n",
            "\n",
            "Episode:  332\n",
            "Current Train Reward:  -1055.4564248761687\n",
            "Current TD Error:  9.562692481709693\n",
            "\n",
            "\n",
            "Episode:  333\n",
            "Current Train Reward:  -1031.7160343121834\n",
            "Current TD Error:  10.170284591666213\n",
            "\n",
            "\n",
            "Episode:  334\n",
            "Current Train Reward:  -1052.7688385159872\n",
            "Current TD Error:  9.018759999861935\n",
            "\n",
            "\n",
            "Episode:  335\n",
            "Current Train Reward:  -1048.6935843589001\n",
            "Current TD Error:  9.100601472698793\n",
            "\n",
            "\n",
            "Episode:  336\n",
            "Current Train Reward:  -1045.2238772455496\n",
            "Current TD Error:  11.507955258584376\n",
            "\n",
            "\n",
            "Episode:  337\n",
            "Current Train Reward:  -1057.5235206945574\n",
            "Current TD Error:  9.754703313405969\n",
            "\n",
            "\n",
            "Episode:  338\n",
            "Current Train Reward:  -1071.5369938019148\n",
            "Current TD Error:  7.086987567740939\n",
            "\n",
            "\n",
            "Episode:  339\n",
            "Current Train Reward:  -1047.0583750945611\n",
            "Current TD Error:  9.667378544102792\n",
            "\n",
            "\n",
            "Episode:  340\n",
            "Current Train Reward:  -1038.7763807042031\n",
            "Current TD Error:  11.787543022077388\n",
            "\n",
            "\n",
            "Episode:  341\n",
            "Current Train Reward:  -1037.2061998197123\n",
            "Current TD Error:  15.771475313106887\n",
            "\n",
            "\n",
            "Episode:  342\n",
            "Current Train Reward:  -1056.3273710498163\n",
            "Current TD Error:  9.941082300765444\n",
            "\n",
            "\n",
            "Episode:  343\n",
            "Current Train Reward:  -1046.6023903424377\n",
            "Current TD Error:  8.708734768101255\n",
            "\n",
            "\n",
            "Episode:  344\n",
            "Current Train Reward:  -1037.3593678390007\n",
            "Current TD Error:  10.091543154399389\n",
            "\n",
            "\n",
            "Episode:  345\n",
            "Current Train Reward:  -1041.449677755744\n",
            "Current TD Error:  9.520683469064629\n",
            "\n",
            "\n",
            "Episode:  346\n",
            "Current Train Reward:  -1073.995147611185\n",
            "Current TD Error:  9.506970794014398\n",
            "\n",
            "\n",
            "Episode:  347\n",
            "Current Train Reward:  -1053.509644021172\n",
            "Current TD Error:  7.49017741055615\n",
            "\n",
            "\n",
            "Episode:  348\n",
            "Current Train Reward:  -1033.6575370861462\n",
            "Current TD Error:  10.29358760969251\n",
            "\n",
            "\n",
            "Episode:  349\n",
            "Current Train Reward:  -1052.98239240553\n",
            "Current TD Error:  9.372609686425484\n",
            "\n",
            "\n",
            "Episode:  350\n",
            "Current Train Reward:  -1071.887292643177\n",
            "Current TD Error:  9.069604078735011\n",
            "\n",
            "\n",
            "Episode:  351\n",
            "Current Train Reward:  -1054.663894767654\n",
            "Current TD Error:  10.277781497338989\n",
            "\n",
            "\n",
            "Episode:  352\n",
            "Current Train Reward:  -1037.2750549552356\n",
            "Current TD Error:  8.825436321869175\n",
            "\n",
            "\n",
            "Episode:  353\n",
            "Current Train Reward:  -1057.9289097821459\n",
            "Current TD Error:  10.533051843337576\n",
            "\n",
            "\n",
            "Episode:  354\n",
            "Current Train Reward:  -1032.2258200901965\n",
            "Current TD Error:  11.563957015349718\n",
            "\n",
            "\n",
            "Episode:  355\n",
            "Current Train Reward:  -1056.9422942880908\n",
            "Current TD Error:  6.91071364402346\n",
            "\n",
            "\n",
            "Episode:  356\n",
            "Current Train Reward:  -1045.5062633785597\n",
            "Current TD Error:  9.28654063054438\n",
            "\n",
            "\n",
            "Episode:  357\n",
            "Current Train Reward:  -1041.5322752323918\n",
            "Current TD Error:  9.243222163212616\n",
            "\n",
            "\n",
            "Episode:  358\n",
            "Current Train Reward:  -1036.1238755648446\n",
            "Current TD Error:  9.539057256896179\n",
            "\n",
            "\n",
            "Episode:  359\n",
            "Current Train Reward:  -1045.7776370359045\n",
            "Current TD Error:  9.348353293435638\n",
            "\n",
            "\n",
            "Episode:  360\n",
            "Current Train Reward:  -1023.9490203488377\n",
            "Current TD Error:  9.745600365093084\n",
            "\n",
            "\n",
            "Episode:  361\n",
            "Current Train Reward:  -1037.2815228995357\n",
            "Current TD Error:  9.305889236725971\n",
            "\n",
            "\n",
            "Episode:  362\n",
            "Current Train Reward:  -1054.8616405359207\n",
            "Current TD Error:  10.863798511985294\n",
            "\n",
            "\n",
            "Episode:  363\n",
            "Current Train Reward:  -1033.8013273038541\n",
            "Current TD Error:  9.653013374673602\n",
            "\n",
            "\n",
            "Episode:  364\n",
            "Current Train Reward:  -1035.8962893079843\n",
            "Current TD Error:  10.613570593275309\n",
            "\n",
            "\n",
            "Episode:  365\n",
            "Current Train Reward:  -1028.7475542077404\n",
            "Current TD Error:  10.150644344053266\n",
            "\n",
            "\n",
            "Episode:  366\n",
            "Current Train Reward:  -1057.3709600229222\n",
            "Current TD Error:  8.993087919070888\n",
            "\n",
            "\n",
            "Episode:  367\n",
            "Current Train Reward:  -1054.0158977867611\n",
            "Current TD Error:  9.66532445878637\n",
            "\n",
            "\n",
            "Episode:  368\n",
            "Current Train Reward:  -1066.7801680928808\n",
            "Current TD Error:  10.885429366545\n",
            "\n",
            "\n",
            "Episode:  369\n",
            "Current Train Reward:  -1030.1985124980522\n",
            "Current TD Error:  9.791631958775147\n",
            "\n",
            "\n",
            "Episode:  370\n",
            "Current Train Reward:  -1026.2308087301355\n",
            "Current TD Error:  9.299231981170427\n",
            "\n",
            "\n",
            "Episode:  371\n",
            "Current Train Reward:  -1049.8711881174743\n",
            "Current TD Error:  9.746910253881596\n",
            "\n",
            "\n",
            "Episode:  372\n",
            "Current Train Reward:  -1033.4883003432064\n",
            "Current TD Error:  8.46686379442706\n",
            "\n",
            "\n",
            "Episode:  373\n",
            "Current Train Reward:  -1065.614843100896\n",
            "Current TD Error:  12.225301897860605\n",
            "\n",
            "\n",
            "Episode:  374\n",
            "Current Train Reward:  -1044.7301412584516\n",
            "Current TD Error:  9.66017143397882\n",
            "\n",
            "\n",
            "Episode:  375\n",
            "Current Train Reward:  -1060.343545001986\n",
            "Current TD Error:  7.483089149228618\n",
            "\n",
            "\n",
            "Episode:  376\n",
            "Current Train Reward:  -1052.9387971369065\n",
            "Current TD Error:  8.63650563955684\n",
            "\n",
            "\n",
            "Episode:  377\n",
            "Current Train Reward:  -1050.2553673958935\n",
            "Current TD Error:  7.513238830834859\n",
            "\n",
            "\n",
            "Episode:  378\n",
            "Current Train Reward:  -1044.4923131721669\n",
            "Current TD Error:  7.272900558004111\n",
            "\n",
            "\n",
            "Episode:  379\n",
            "Current Train Reward:  -1053.5880019129108\n",
            "Current TD Error:  7.067857458292959\n",
            "\n",
            "\n",
            "Episode:  380\n",
            "Current Train Reward:  -1065.2057833768836\n",
            "Current TD Error:  9.787989528684065\n",
            "\n",
            "\n",
            "Episode:  381\n",
            "Current Train Reward:  -1026.934653252664\n",
            "Current TD Error:  9.245753104893245\n",
            "\n",
            "\n",
            "Episode:  382\n",
            "Current Train Reward:  -1038.8999093851623\n",
            "Current TD Error:  9.541217795219989\n",
            "\n",
            "\n",
            "Episode:  383\n",
            "Current Train Reward:  -1059.559171476488\n",
            "Current TD Error:  9.492292550600439\n",
            "\n",
            "\n",
            "Episode:  384\n",
            "Current Train Reward:  -1047.537080949356\n",
            "Current TD Error:  9.401276299623836\n",
            "\n",
            "\n",
            "Episode:  385\n",
            "Current Train Reward:  -1059.9009905741843\n",
            "Current TD Error:  10.11486723591648\n",
            "\n",
            "\n",
            "Episode:  386\n",
            "Current Train Reward:  -1029.3133886943845\n",
            "Current TD Error:  9.84484770759208\n",
            "\n",
            "\n",
            "Episode:  387\n",
            "Current Train Reward:  -1050.5681153092416\n",
            "Current TD Error:  7.9908632940745195\n",
            "\n",
            "\n",
            "Episode:  388\n",
            "Current Train Reward:  -1056.189871440774\n",
            "Current TD Error:  7.512342034166856\n",
            "\n",
            "\n",
            "Episode:  389\n",
            "Current Train Reward:  -1043.2279125196067\n",
            "Current TD Error:  7.459862095222859\n",
            "\n",
            "\n",
            "Episode:  390\n",
            "Current Train Reward:  -1044.8975669302906\n",
            "Current TD Error:  8.423793063944109\n",
            "\n",
            "\n",
            "Episode:  391\n",
            "Current Train Reward:  -1070.1702182529566\n",
            "Current TD Error:  9.141815462777178\n",
            "\n",
            "\n",
            "Episode:  392\n",
            "Current Train Reward:  -1045.4085497508224\n",
            "Current TD Error:  8.404729109253044\n",
            "\n",
            "\n",
            "Episode:  393\n",
            "Current Train Reward:  -1024.6506710663475\n",
            "Current TD Error:  7.328178056447939\n",
            "\n",
            "\n",
            "Episode:  394\n",
            "Current Train Reward:  -1056.2305947486668\n",
            "Current TD Error:  8.124219743230263\n",
            "\n",
            "\n",
            "Episode:  395\n",
            "Current Train Reward:  -1040.739666704451\n",
            "Current TD Error:  8.781421440049309\n",
            "\n",
            "\n",
            "Episode:  396\n",
            "Current Train Reward:  -1046.054587193787\n",
            "Current TD Error:  7.607242252071453\n",
            "\n",
            "\n",
            "Episode:  397\n",
            "Current Train Reward:  -1053.9688273170025\n",
            "Current TD Error:  8.971927012413815\n",
            "\n",
            "\n",
            "Episode:  398\n",
            "Current Train Reward:  -1043.8194271824286\n",
            "Current TD Error:  9.035995915177025\n",
            "\n",
            "\n",
            "Episode:  399\n",
            "Current Train Reward:  -1023.1651876452249\n",
            "Current TD Error:  8.271110314835074\n",
            "\n",
            "\n",
            "Episode:  400\n",
            "Current Train Reward:  -1042.7019804937347\n",
            "Current TD Error:  12.683218802217466\n",
            "\n",
            "\n",
            "Current Test Reward:  -1086.8138992340282\n",
            "Episode:  401\n",
            "Current Train Reward:  -1051.7118338525167\n",
            "Current TD Error:  7.899038901783833\n",
            "\n",
            "\n",
            "Episode:  402\n",
            "Current Train Reward:  -1031.359369405557\n",
            "Current TD Error:  7.622979809403194\n",
            "\n",
            "\n",
            "Episode:  403\n",
            "Current Train Reward:  -1046.4382533280832\n",
            "Current TD Error:  8.033916646481295\n",
            "\n",
            "\n",
            "Episode:  404\n",
            "Current Train Reward:  -1026.1214749821909\n",
            "Current TD Error:  9.852499806681491\n",
            "\n",
            "\n",
            "Episode:  405\n",
            "Current Train Reward:  -1032.1160676251425\n",
            "Current TD Error:  8.798109401909777\n",
            "\n",
            "\n",
            "Episode:  406\n",
            "Current Train Reward:  -1050.0153474502633\n",
            "Current TD Error:  8.525959498765177\n",
            "\n",
            "\n",
            "Episode:  407\n",
            "Current Train Reward:  -1026.624718942666\n",
            "Current TD Error:  9.280950406931241\n",
            "\n",
            "\n",
            "Episode:  408\n",
            "Current Train Reward:  -1039.6478305332116\n",
            "Current TD Error:  8.512825547025066\n",
            "\n",
            "\n",
            "Episode:  409\n",
            "Current Train Reward:  -1056.6520645645323\n",
            "Current TD Error:  8.089240773973767\n",
            "\n",
            "\n",
            "Episode:  410\n",
            "Current Train Reward:  -1035.6669769654384\n",
            "Current TD Error:  8.008650811923692\n",
            "\n",
            "\n",
            "Episode:  411\n",
            "Current Train Reward:  -1053.5797501351283\n",
            "Current TD Error:  9.169431328011136\n",
            "\n",
            "\n",
            "Episode:  412\n",
            "Current Train Reward:  -1040.86352067268\n",
            "Current TD Error:  7.811326756890975\n",
            "\n",
            "\n",
            "Episode:  413\n",
            "Current Train Reward:  -1104.2637349387273\n",
            "Current TD Error:  6.53457977747299\n",
            "\n",
            "\n",
            "Episode:  414\n",
            "Current Train Reward:  -1033.0927843758855\n",
            "Current TD Error:  11.645107334543773\n",
            "\n",
            "\n",
            "Episode:  415\n",
            "Current Train Reward:  -1041.9463681124496\n",
            "Current TD Error:  7.565935934070408\n",
            "\n",
            "\n",
            "Episode:  416\n",
            "Current Train Reward:  -1060.605674761982\n",
            "Current TD Error:  10.074496427412551\n",
            "\n",
            "\n",
            "Episode:  417\n",
            "Current Train Reward:  -1063.5945326264925\n",
            "Current TD Error:  7.178336885945025\n",
            "\n",
            "\n",
            "Episode:  418\n",
            "Current Train Reward:  -1025.223735014092\n",
            "Current TD Error:  8.21012791907389\n",
            "\n",
            "\n",
            "Episode:  419\n",
            "Current Train Reward:  -1045.3337258365866\n",
            "Current TD Error:  9.37890741222202\n",
            "\n",
            "\n",
            "Episode:  420\n",
            "Current Train Reward:  -1022.5337529067635\n",
            "Current TD Error:  6.837732289679782\n",
            "\n",
            "\n",
            "Episode:  421\n",
            "Current Train Reward:  -1052.4134110904754\n",
            "Current TD Error:  8.186711606302316\n",
            "\n",
            "\n",
            "Episode:  422\n",
            "Current Train Reward:  -1035.5162990135013\n",
            "Current TD Error:  9.644564613946063\n",
            "\n",
            "\n",
            "Episode:  423\n",
            "Current Train Reward:  -1058.9808118518806\n",
            "Current TD Error:  7.041487645206711\n",
            "\n",
            "\n",
            "Episode:  424\n",
            "Current Train Reward:  -1052.7300528556684\n",
            "Current TD Error:  6.59743738711372\n",
            "\n",
            "\n",
            "Episode:  425\n",
            "Current Train Reward:  -1045.6574412117247\n",
            "Current TD Error:  6.908631356737698\n",
            "\n",
            "\n",
            "Episode:  426\n",
            "Current Train Reward:  -1048.2118745928194\n",
            "Current TD Error:  7.60512062237021\n",
            "\n",
            "\n",
            "Episode:  427\n",
            "Current Train Reward:  -1044.0775418746673\n",
            "Current TD Error:  9.398173878406954\n",
            "\n",
            "\n",
            "Episode:  428\n",
            "Current Train Reward:  -1052.8927220753367\n",
            "Current TD Error:  6.371823001856821\n",
            "\n",
            "\n",
            "Episode:  429\n",
            "Current Train Reward:  -1067.361365993814\n",
            "Current TD Error:  7.839318339855676\n",
            "\n",
            "\n",
            "Episode:  430\n",
            "Current Train Reward:  -1038.8321987539036\n",
            "Current TD Error:  7.045451815675372\n",
            "\n",
            "\n",
            "Episode:  431\n",
            "Current Train Reward:  -1038.9951527507533\n",
            "Current TD Error:  7.335825282446803\n",
            "\n",
            "\n",
            "Episode:  432\n",
            "Current Train Reward:  -1035.4932356476033\n",
            "Current TD Error:  7.799227418834683\n",
            "\n",
            "\n",
            "Episode:  433\n",
            "Current Train Reward:  -1035.9656554970368\n",
            "Current TD Error:  8.448925566106341\n",
            "\n",
            "\n",
            "Episode:  434\n",
            "Current Train Reward:  -1055.9074322397435\n",
            "Current TD Error:  8.684861803786639\n",
            "\n",
            "\n",
            "Episode:  435\n",
            "Current Train Reward:  -1038.9961301829906\n",
            "Current TD Error:  7.233663434876169\n",
            "\n",
            "\n",
            "Episode:  436\n",
            "Current Train Reward:  -1029.3417813430708\n",
            "Current TD Error:  8.71917448047093\n",
            "\n",
            "\n",
            "Episode:  437\n",
            "Current Train Reward:  -1058.2665794924635\n",
            "Current TD Error:  9.426591332559942\n",
            "\n",
            "\n",
            "Episode:  438\n",
            "Current Train Reward:  -1047.6224129868651\n",
            "Current TD Error:  8.967803687571132\n",
            "\n",
            "\n",
            "Episode:  439\n",
            "Current Train Reward:  -1046.335510449708\n",
            "Current TD Error:  7.213446061674445\n",
            "\n",
            "\n",
            "Episode:  440\n",
            "Current Train Reward:  -1032.4281742313224\n",
            "Current TD Error:  7.9905205092273075\n",
            "\n",
            "\n",
            "Episode:  441\n",
            "Current Train Reward:  -1031.815567638045\n",
            "Current TD Error:  8.725545816864429\n",
            "\n",
            "\n",
            "Episode:  442\n",
            "Current Train Reward:  -1049.080059759173\n",
            "Current TD Error:  7.358628119762106\n",
            "\n",
            "\n",
            "Episode:  443\n",
            "Current Train Reward:  -1022.2474462509334\n",
            "Current TD Error:  7.391693043131587\n",
            "\n",
            "\n",
            "Episode:  444\n",
            "Current Train Reward:  -1031.9509657879667\n",
            "Current TD Error:  8.569354550911218\n",
            "\n",
            "\n",
            "Episode:  445\n",
            "Current Train Reward:  -1031.2448152999189\n",
            "Current TD Error:  7.397794657437708\n",
            "\n",
            "\n",
            "Episode:  446\n",
            "Current Train Reward:  -1034.783192252448\n",
            "Current TD Error:  9.622493104646892\n",
            "\n",
            "\n",
            "Episode:  447\n",
            "Current Train Reward:  -1056.3223779773753\n",
            "Current TD Error:  7.545060938541444\n",
            "\n",
            "\n",
            "Episode:  448\n",
            "Current Train Reward:  -1063.7636816328186\n",
            "Current TD Error:  7.403437198709098\n",
            "\n",
            "\n",
            "Episode:  449\n",
            "Current Train Reward:  -1032.2998208720203\n",
            "Current TD Error:  7.356682864146397\n",
            "\n",
            "\n",
            "Episode:  450\n",
            "Current Train Reward:  -1046.5340409578662\n",
            "Current TD Error:  6.178801272843633\n",
            "\n",
            "\n",
            "Episode:  451\n",
            "Current Train Reward:  -1045.368596123664\n",
            "Current TD Error:  8.903865552764842\n",
            "\n",
            "\n",
            "Episode:  452\n",
            "Current Train Reward:  -1032.7414864770424\n",
            "Current TD Error:  8.538823794242084\n",
            "\n",
            "\n",
            "Episode:  453\n",
            "Current Train Reward:  -1046.9615211135267\n",
            "Current TD Error:  7.483635082219811\n",
            "\n",
            "\n",
            "Episode:  454\n",
            "Current Train Reward:  -1032.32995623151\n",
            "Current TD Error:  8.634460042254297\n",
            "\n",
            "\n",
            "Episode:  455\n",
            "Current Train Reward:  -1054.4762862635919\n",
            "Current TD Error:  7.225131493226745\n",
            "\n",
            "\n",
            "Episode:  456\n",
            "Current Train Reward:  -1035.0610815663301\n",
            "Current TD Error:  8.043459194141565\n",
            "\n",
            "\n",
            "Episode:  457\n",
            "Current Train Reward:  -1055.547420133454\n",
            "Current TD Error:  8.246358534699503\n",
            "\n",
            "\n",
            "Episode:  458\n",
            "Current Train Reward:  -1037.783228662988\n",
            "Current TD Error:  6.181518031318944\n",
            "\n",
            "\n",
            "Episode:  459\n",
            "Current Train Reward:  -1043.2231761444132\n",
            "Current TD Error:  7.90370293288825\n",
            "\n",
            "\n",
            "Episode:  460\n",
            "Current Train Reward:  -1062.0087946764434\n",
            "Current TD Error:  8.888763430023866\n",
            "\n",
            "\n",
            "Episode:  461\n",
            "Current Train Reward:  -1040.0932211364038\n",
            "Current TD Error:  11.053810725639257\n",
            "\n",
            "\n",
            "Episode:  462\n",
            "Current Train Reward:  -1050.5109796951533\n",
            "Current TD Error:  6.002599840670238\n",
            "\n",
            "\n",
            "Episode:  463\n",
            "Current Train Reward:  -1049.961963254611\n",
            "Current TD Error:  6.696468064497902\n",
            "\n",
            "\n",
            "Episode:  464\n",
            "Current Train Reward:  -1045.2854682769346\n",
            "Current TD Error:  6.778034946735743\n",
            "\n",
            "\n",
            "Episode:  465\n",
            "Current Train Reward:  -1049.9125827835474\n",
            "Current TD Error:  9.15369822447093\n",
            "\n",
            "\n",
            "Episode:  466\n",
            "Current Train Reward:  -1022.2266955543519\n",
            "Current TD Error:  9.122819868299736\n",
            "\n",
            "\n",
            "Episode:  467\n",
            "Current Train Reward:  -1056.1297897558134\n",
            "Current TD Error:  7.202963267927664\n",
            "\n",
            "\n",
            "Episode:  468\n",
            "Current Train Reward:  -1042.7776898084828\n",
            "Current TD Error:  7.311632989092521\n",
            "\n",
            "\n",
            "Episode:  469\n",
            "Current Train Reward:  -1108.225552017376\n",
            "Current TD Error:  4.836960013614084\n",
            "\n",
            "\n",
            "Episode:  470\n",
            "Current Train Reward:  -1025.2592121291098\n",
            "Current TD Error:  5.385880255218855\n",
            "\n",
            "\n",
            "Episode:  471\n",
            "Current Train Reward:  -1037.4443017659642\n",
            "Current TD Error:  8.158577743069651\n",
            "\n",
            "\n",
            "Episode:  472\n",
            "Current Train Reward:  -1042.0057980833724\n",
            "Current TD Error:  7.157139039099564\n",
            "\n",
            "\n",
            "Episode:  473\n",
            "Current Train Reward:  -1069.0846496330007\n",
            "Current TD Error:  9.429382122674314\n",
            "\n",
            "\n",
            "Episode:  474\n",
            "Current Train Reward:  -1049.0489755875374\n",
            "Current TD Error:  6.9797568450335215\n",
            "\n",
            "\n",
            "Episode:  475\n",
            "Current Train Reward:  -1037.7699096486356\n",
            "Current TD Error:  7.290929215024719\n",
            "\n",
            "\n",
            "Episode:  476\n",
            "Current Train Reward:  -1038.9865021264625\n",
            "Current TD Error:  6.678175152736567\n",
            "\n",
            "\n",
            "Episode:  477\n",
            "Current Train Reward:  -1189.2456800851241\n",
            "Current TD Error:  5.735832556939601\n",
            "\n",
            "\n",
            "Episode:  478\n",
            "Current Train Reward:  -1038.3704562683736\n",
            "Current TD Error:  8.636010255270257\n",
            "\n",
            "\n",
            "Episode:  479\n",
            "Current Train Reward:  -1059.1313523088988\n",
            "Current TD Error:  11.097980081317184\n",
            "\n",
            "\n",
            "Episode:  480\n",
            "Current Train Reward:  -1041.1759807088383\n",
            "Current TD Error:  7.53349742533108\n",
            "\n",
            "\n",
            "Episode:  481\n",
            "Current Train Reward:  -1034.620326013703\n",
            "Current TD Error:  7.023184610951735\n",
            "\n",
            "\n",
            "Episode:  482\n",
            "Current Train Reward:  -1053.1312083330827\n",
            "Current TD Error:  4.950809163050421\n",
            "\n",
            "\n",
            "Episode:  483\n",
            "Current Train Reward:  -1021.8925754383273\n",
            "Current TD Error:  6.457819918205874\n",
            "\n",
            "\n",
            "Episode:  484\n",
            "Current Train Reward:  -1048.9445427574017\n",
            "Current TD Error:  10.263638655379346\n",
            "\n",
            "\n",
            "Episode:  485\n",
            "Current Train Reward:  -1105.4943225342581\n",
            "Current TD Error:  5.9296150234052885\n",
            "\n",
            "\n",
            "Episode:  486\n",
            "Current Train Reward:  -1025.5655377486412\n",
            "Current TD Error:  8.31429303801209\n",
            "\n",
            "\n",
            "Episode:  487\n",
            "Current Train Reward:  -1049.9264541732161\n",
            "Current TD Error:  11.073577448609484\n",
            "\n",
            "\n",
            "Episode:  488\n",
            "Current Train Reward:  -1075.1647839321624\n",
            "Current TD Error:  7.2255354032421595\n",
            "\n",
            "\n",
            "Episode:  489\n",
            "Current Train Reward:  -1058.7680231161394\n",
            "Current TD Error:  7.3694018264235135\n",
            "\n",
            "\n",
            "Episode:  490\n",
            "Current Train Reward:  -1040.8244200336183\n",
            "Current TD Error:  9.58620400610414\n",
            "\n",
            "\n",
            "Episode:  491\n",
            "Current Train Reward:  -1071.6280691435902\n",
            "Current TD Error:  6.310239544879316\n",
            "\n",
            "\n",
            "Episode:  492\n",
            "Current Train Reward:  -1048.9141347978275\n",
            "Current TD Error:  7.65885330858053\n",
            "\n",
            "\n",
            "Episode:  493\n",
            "Current Train Reward:  -1041.1612231393472\n",
            "Current TD Error:  8.183934936782755\n",
            "\n",
            "\n",
            "Episode:  494\n",
            "Current Train Reward:  -1049.9516971692162\n",
            "Current TD Error:  6.500242282484168\n",
            "\n",
            "\n",
            "Episode:  495\n",
            "Current Train Reward:  -1041.3627500850264\n",
            "Current TD Error:  5.7811939736871505\n",
            "\n",
            "\n",
            "Episode:  496\n",
            "Current Train Reward:  -1051.7095501990348\n",
            "Current TD Error:  6.41010535544002\n",
            "\n",
            "\n",
            "Episode:  497\n",
            "Current Train Reward:  -1039.2884036877833\n",
            "Current TD Error:  10.01469681419381\n",
            "\n",
            "\n",
            "Episode:  498\n",
            "Current Train Reward:  -1040.8432789746557\n",
            "Current TD Error:  7.206532403289747\n",
            "\n",
            "\n",
            "Episode:  499\n",
            "Current Train Reward:  -1031.8524491784592\n",
            "Current TD Error:  7.653129034998522\n",
            "\n",
            "\n",
            "Episode:  500\n",
            "Current Train Reward:  -1049.8827074267224\n",
            "Current TD Error:  6.646945639495238\n",
            "\n",
            "\n",
            "Current Test Reward:  -1152.9913845434717\n",
            "Episode:  501\n",
            "Current Train Reward:  -1075.8323612137006\n",
            "Current TD Error:  5.862393201089007\n",
            "\n",
            "\n",
            "Episode:  502\n",
            "Current Train Reward:  -1036.4985330238924\n",
            "Current TD Error:  7.316370231123277\n",
            "\n",
            "\n",
            "Episode:  503\n",
            "Current Train Reward:  -1067.94817610135\n",
            "Current TD Error:  7.109271102995732\n",
            "\n",
            "\n",
            "Episode:  504\n",
            "Current Train Reward:  -1031.1318183671008\n",
            "Current TD Error:  7.822471961169521\n",
            "\n",
            "\n",
            "Episode:  505\n",
            "Current Train Reward:  -1065.0767545407114\n",
            "Current TD Error:  5.811226837648069\n",
            "\n",
            "\n",
            "Episode:  506\n",
            "Current Train Reward:  -1042.6934193109976\n",
            "Current TD Error:  6.661319527165629\n",
            "\n",
            "\n",
            "Episode:  507\n",
            "Current Train Reward:  -1080.0079217245438\n",
            "Current TD Error:  5.396952768719121\n",
            "\n",
            "\n",
            "Episode:  508\n",
            "Current Train Reward:  -1079.4041085507483\n",
            "Current TD Error:  7.178174612846055\n",
            "\n",
            "\n",
            "Episode:  509\n",
            "Current Train Reward:  -1082.5330050454395\n",
            "Current TD Error:  5.759823960512174\n",
            "\n",
            "\n",
            "Episode:  510\n",
            "Current Train Reward:  -1046.0313642111294\n",
            "Current TD Error:  7.706191668808451\n",
            "\n",
            "\n",
            "Episode:  511\n",
            "Current Train Reward:  -1034.50281356147\n",
            "Current TD Error:  7.987404133062095\n",
            "\n",
            "\n",
            "Episode:  512\n",
            "Current Train Reward:  -1046.0057395077572\n",
            "Current TD Error:  6.165404810210847\n",
            "\n",
            "\n",
            "Episode:  513\n",
            "Current Train Reward:  -1054.9256603406452\n",
            "Current TD Error:  7.196731933067815\n",
            "\n",
            "\n",
            "Episode:  514\n",
            "Current Train Reward:  -1066.5333998610572\n",
            "Current TD Error:  7.14975766803533\n",
            "\n",
            "\n",
            "Episode:  515\n",
            "Current Train Reward:  -1063.3605738144265\n",
            "Current TD Error:  5.26559978230821\n",
            "\n",
            "\n",
            "Episode:  516\n",
            "Current Train Reward:  -1016.6767213443635\n",
            "Current TD Error:  5.930040601883858\n",
            "\n",
            "\n",
            "Episode:  517\n",
            "Current Train Reward:  -1056.6952702326653\n",
            "Current TD Error:  5.765888093275312\n",
            "\n",
            "\n",
            "Episode:  518\n",
            "Current Train Reward:  -1090.9707210668719\n",
            "Current TD Error:  5.303824613885937\n",
            "\n",
            "\n",
            "Episode:  519\n",
            "Current Train Reward:  -1064.0361217287098\n",
            "Current TD Error:  6.388731947491544\n",
            "\n",
            "\n",
            "Episode:  520\n",
            "Current Train Reward:  -1058.7024302335485\n",
            "Current TD Error:  6.450199544206269\n",
            "\n",
            "\n",
            "Episode:  521\n",
            "Current Train Reward:  -1094.0459990494526\n",
            "Current TD Error:  5.512592391604215\n",
            "\n",
            "\n",
            "Episode:  522\n",
            "Current Train Reward:  -1057.0565320735077\n",
            "Current TD Error:  7.794555032592456\n",
            "\n",
            "\n",
            "Episode:  523\n",
            "Current Train Reward:  -1047.0060328607383\n",
            "Current TD Error:  8.66051293765022\n",
            "\n",
            "\n",
            "Episode:  524\n",
            "Current Train Reward:  -1049.0976703229571\n",
            "Current TD Error:  6.203970707780436\n",
            "\n",
            "\n",
            "Episode:  525\n",
            "Current Train Reward:  -1046.2490169201233\n",
            "Current TD Error:  7.065510279812924\n",
            "\n",
            "\n",
            "Episode:  526\n",
            "Current Train Reward:  -1037.247957099018\n",
            "Current TD Error:  7.65752394383725\n",
            "\n",
            "\n",
            "Episode:  527\n",
            "Current Train Reward:  -1053.3369143291422\n",
            "Current TD Error:  7.211540629995298\n",
            "\n",
            "\n",
            "Episode:  528\n",
            "Current Train Reward:  -1044.1502079463621\n",
            "Current TD Error:  6.822273087904278\n",
            "\n",
            "\n",
            "Episode:  529\n",
            "Current Train Reward:  -1055.7798956414933\n",
            "Current TD Error:  6.868073416732695\n",
            "\n",
            "\n",
            "Episode:  530\n",
            "Current Train Reward:  -1039.686899372458\n",
            "Current TD Error:  5.0696269422008156\n",
            "\n",
            "\n",
            "Episode:  531\n",
            "Current Train Reward:  -1044.8558789620467\n",
            "Current TD Error:  7.689972886917548\n",
            "\n",
            "\n",
            "Episode:  532\n",
            "Current Train Reward:  -1039.5191256283308\n",
            "Current TD Error:  7.301411098018072\n",
            "\n",
            "\n",
            "Episode:  533\n",
            "Current Train Reward:  -1040.0462050772899\n",
            "Current TD Error:  9.76379324109357\n",
            "\n",
            "\n",
            "Episode:  534\n",
            "Current Train Reward:  -1122.204597734596\n",
            "Current TD Error:  5.803553375828338\n",
            "\n",
            "\n",
            "Episode:  535\n",
            "Current Train Reward:  -1044.2301135059365\n",
            "Current TD Error:  5.686724149382722\n",
            "\n",
            "\n",
            "Episode:  536\n",
            "Current Train Reward:  -1061.1168133191977\n",
            "Current TD Error:  4.768993788583501\n",
            "\n",
            "\n",
            "Episode:  537\n",
            "Current Train Reward:  -1105.2573233094017\n",
            "Current TD Error:  5.468417969469524\n",
            "\n",
            "\n",
            "Episode:  538\n",
            "Current Train Reward:  -1055.2152742172593\n",
            "Current TD Error:  6.777974425738809\n",
            "\n",
            "\n",
            "Episode:  539\n",
            "Current Train Reward:  -1052.4620976821627\n",
            "Current TD Error:  5.0372906694363015\n",
            "\n",
            "\n",
            "Episode:  540\n",
            "Current Train Reward:  -1054.2961502937721\n",
            "Current TD Error:  7.96964189810199\n",
            "\n",
            "\n",
            "Episode:  541\n",
            "Current Train Reward:  -1050.6140001086244\n",
            "Current TD Error:  7.475961551369472\n",
            "\n",
            "\n",
            "Episode:  542\n",
            "Current Train Reward:  -1029.9330234858567\n",
            "Current TD Error:  6.197039713214948\n",
            "\n",
            "\n",
            "Episode:  543\n",
            "Current Train Reward:  -1085.054654534412\n",
            "Current TD Error:  5.991908802094179\n",
            "\n",
            "\n",
            "Episode:  544\n",
            "Current Train Reward:  -1030.3799934110662\n",
            "Current TD Error:  7.582420259022643\n",
            "\n",
            "\n",
            "Episode:  545\n",
            "Current Train Reward:  -1027.5982870078028\n",
            "Current TD Error:  8.116885402693564\n",
            "\n",
            "\n",
            "Episode:  546\n",
            "Current Train Reward:  -1040.9271271213488\n",
            "Current TD Error:  7.436532891625656\n",
            "\n",
            "\n",
            "Episode:  547\n",
            "Current Train Reward:  -1059.2968590065288\n",
            "Current TD Error:  6.775891204963676\n",
            "\n",
            "\n",
            "Episode:  548\n",
            "Current Train Reward:  -1042.4892311843848\n",
            "Current TD Error:  6.439201566706223\n",
            "\n",
            "\n",
            "Episode:  549\n",
            "Current Train Reward:  -1042.535402846258\n",
            "Current TD Error:  5.908627558988878\n",
            "\n",
            "\n",
            "Episode:  550\n",
            "Current Train Reward:  -1061.2552400622737\n",
            "Current TD Error:  6.8584287587192945\n",
            "\n",
            "\n",
            "Episode:  551\n",
            "Current Train Reward:  -1055.5912791364178\n",
            "Current TD Error:  5.533963522849709\n",
            "\n",
            "\n",
            "Episode:  552\n",
            "Current Train Reward:  -1104.6523835277253\n",
            "Current TD Error:  6.075873513141812\n",
            "\n",
            "\n",
            "Episode:  553\n",
            "Current Train Reward:  -1052.229283824522\n",
            "Current TD Error:  6.166290164853781\n",
            "\n",
            "\n",
            "Episode:  554\n",
            "Current Train Reward:  -1032.89499923308\n",
            "Current TD Error:  7.08712533798508\n",
            "\n",
            "\n",
            "Episode:  555\n",
            "Current Train Reward:  -1053.6142215281834\n",
            "Current TD Error:  5.606534280645583\n",
            "\n",
            "\n",
            "Episode:  556\n",
            "Current Train Reward:  -1054.4673727640145\n",
            "Current TD Error:  8.93466393931007\n",
            "\n",
            "\n",
            "Episode:  557\n",
            "Current Train Reward:  -1093.1844539098956\n",
            "Current TD Error:  5.904872870949695\n",
            "\n",
            "\n",
            "Episode:  558\n",
            "Current Train Reward:  -1063.1906586021469\n",
            "Current TD Error:  5.220886840829563\n",
            "\n",
            "\n",
            "Episode:  559\n",
            "Current Train Reward:  -1044.2851387399592\n",
            "Current TD Error:  6.454160269695737\n",
            "\n",
            "\n",
            "Episode:  560\n",
            "Current Train Reward:  -1032.2982286377696\n",
            "Current TD Error:  5.003984973090514\n",
            "\n",
            "\n",
            "Episode:  561\n",
            "Current Train Reward:  -1114.0553465468954\n",
            "Current TD Error:  4.764260452866544\n",
            "\n",
            "\n",
            "Episode:  562\n",
            "Current Train Reward:  -1084.0106087680044\n",
            "Current TD Error:  5.671704895735232\n",
            "\n",
            "\n",
            "Episode:  563\n",
            "Current Train Reward:  -1051.339493149852\n",
            "Current TD Error:  7.688832993959099\n",
            "\n",
            "\n",
            "Episode:  564\n",
            "Current Train Reward:  -1047.4716191222237\n",
            "Current TD Error:  7.946302290370431\n",
            "\n",
            "\n",
            "Episode:  565\n",
            "Current Train Reward:  -1048.8694500812455\n",
            "Current TD Error:  4.475770293700922\n",
            "\n",
            "\n",
            "Episode:  566\n",
            "Current Train Reward:  -1040.1456272065932\n",
            "Current TD Error:  5.311985837227549\n",
            "\n",
            "\n",
            "Episode:  567\n",
            "Current Train Reward:  -1192.2262732318477\n",
            "Current TD Error:  4.941113122028216\n",
            "\n",
            "\n",
            "Episode:  568\n",
            "Current Train Reward:  -1034.0063633350692\n",
            "Current TD Error:  7.102839247086217\n",
            "\n",
            "\n",
            "Episode:  569\n",
            "Current Train Reward:  -1053.2291493662526\n",
            "Current TD Error:  7.744522438587176\n",
            "\n",
            "\n",
            "Episode:  570\n",
            "Current Train Reward:  -1077.2793644304443\n",
            "Current TD Error:  5.601997628933102\n",
            "\n",
            "\n",
            "Episode:  571\n",
            "Current Train Reward:  -1101.3071179889916\n",
            "Current TD Error:  5.636292016527769\n",
            "\n",
            "\n",
            "Episode:  572\n",
            "Current Train Reward:  -1048.6692633383382\n",
            "Current TD Error:  6.590944127826772\n",
            "\n",
            "\n",
            "Episode:  573\n",
            "Current Train Reward:  -1081.737762397018\n",
            "Current TD Error:  6.169669679657597\n",
            "\n",
            "\n",
            "Episode:  574\n",
            "Current Train Reward:  -1025.3967140989482\n",
            "Current TD Error:  6.574680635599487\n",
            "\n",
            "\n",
            "Episode:  575\n",
            "Current Train Reward:  -1117.0035928868185\n",
            "Current TD Error:  5.0483400433278085\n",
            "\n",
            "\n",
            "Episode:  576\n",
            "Current Train Reward:  -1114.5833341972452\n",
            "Current TD Error:  4.648313084784026\n",
            "\n",
            "\n",
            "Episode:  577\n",
            "Current Train Reward:  -1125.1693537270075\n",
            "Current TD Error:  5.567632192021974\n",
            "\n",
            "\n",
            "Episode:  578\n",
            "Current Train Reward:  -1029.2612694177333\n",
            "Current TD Error:  6.691369076980852\n",
            "\n",
            "\n",
            "Episode:  579\n",
            "Current Train Reward:  -1081.428760911655\n",
            "Current TD Error:  5.936204652705355\n",
            "\n",
            "\n",
            "Episode:  580\n",
            "Current Train Reward:  -1062.0681789740477\n",
            "Current TD Error:  8.355082384179987\n",
            "\n",
            "\n",
            "Episode:  581\n",
            "Current Train Reward:  -1068.9368353536395\n",
            "Current TD Error:  5.910553622364176\n",
            "\n",
            "\n",
            "Episode:  582\n",
            "Current Train Reward:  -1079.7198318663334\n",
            "Current TD Error:  6.8990567328965815\n",
            "\n",
            "\n",
            "Episode:  583\n",
            "Current Train Reward:  -1031.843468072456\n",
            "Current TD Error:  7.6957198589125895\n",
            "\n",
            "\n",
            "Episode:  584\n",
            "Current Train Reward:  -1074.3367352773366\n",
            "Current TD Error:  6.013929013603486\n",
            "\n",
            "\n",
            "Episode:  585\n",
            "Current Train Reward:  -1053.4508434218505\n",
            "Current TD Error:  6.670077621676464\n",
            "\n",
            "\n",
            "Episode:  586\n",
            "Current Train Reward:  -1055.912764956403\n",
            "Current TD Error:  6.329249169178143\n",
            "\n",
            "\n",
            "Episode:  587\n",
            "Current Train Reward:  -1116.9899851738285\n",
            "Current TD Error:  5.5495056375083145\n",
            "\n",
            "\n",
            "Episode:  588\n",
            "Current Train Reward:  -1024.7857295662384\n",
            "Current TD Error:  5.77357897275821\n",
            "\n",
            "\n",
            "Episode:  589\n",
            "Current Train Reward:  -1026.0411323280846\n",
            "Current TD Error:  6.497237334113264\n",
            "\n",
            "\n",
            "Episode:  590\n",
            "Current Train Reward:  -1034.8876630906823\n",
            "Current TD Error:  5.426262423422198\n",
            "\n",
            "\n",
            "Episode:  591\n",
            "Current Train Reward:  -1047.2004772093524\n",
            "Current TD Error:  6.9948841977446765\n",
            "\n",
            "\n",
            "Episode:  592\n",
            "Current Train Reward:  -1062.1846247188826\n",
            "Current TD Error:  5.259916850910576\n",
            "\n",
            "\n",
            "Episode:  593\n",
            "Current Train Reward:  -1225.2073721475147\n",
            "Current TD Error:  4.429077249069458\n",
            "\n",
            "\n",
            "Episode:  594\n",
            "Current Train Reward:  -1100.15252799664\n",
            "Current TD Error:  5.125405355351279\n",
            "\n",
            "\n",
            "Episode:  595\n",
            "Current Train Reward:  -1050.0541010286759\n",
            "Current TD Error:  4.7962155022994\n",
            "\n",
            "\n",
            "Episode:  596\n",
            "Current Train Reward:  -1056.0848106278058\n",
            "Current TD Error:  5.274871030176257\n",
            "\n",
            "\n",
            "Episode:  597\n",
            "Current Train Reward:  -1066.2267712494424\n",
            "Current TD Error:  5.256860567227097\n",
            "\n",
            "\n",
            "Episode:  598\n",
            "Current Train Reward:  -1016.4821324966958\n",
            "Current TD Error:  5.488811299137042\n",
            "\n",
            "\n",
            "Episode:  599\n",
            "Current Train Reward:  -1055.076943038551\n",
            "Current TD Error:  10.773397038152202\n",
            "\n",
            "\n",
            "Episode:  600\n",
            "Current Train Reward:  -1179.5409596337518\n",
            "Current TD Error:  5.267499348011432\n",
            "\n",
            "\n",
            "Current Test Reward:  -1150.9148808028394\n",
            "Episode:  601\n",
            "Current Train Reward:  -1031.2007078910099\n",
            "Current TD Error:  5.981881471205854\n",
            "\n",
            "\n",
            "Episode:  602\n",
            "Current Train Reward:  -1075.2830828301305\n",
            "Current TD Error:  5.78988006882746\n",
            "\n",
            "\n",
            "Episode:  603\n",
            "Current Train Reward:  -1112.967038107194\n",
            "Current TD Error:  5.64965440329543\n",
            "\n",
            "\n",
            "Episode:  604\n",
            "Current Train Reward:  -1045.1228853306407\n",
            "Current TD Error:  5.447915693400715\n",
            "\n",
            "\n",
            "Episode:  605\n",
            "Current Train Reward:  -1028.201893698695\n",
            "Current TD Error:  7.604137309439594\n",
            "\n",
            "\n",
            "Episode:  606\n",
            "Current Train Reward:  -1060.247259163598\n",
            "Current TD Error:  7.4185284027617975\n",
            "\n",
            "\n",
            "Episode:  607\n",
            "Current Train Reward:  -1052.9561207122379\n",
            "Current TD Error:  7.4299098423198044\n",
            "\n",
            "\n",
            "Episode:  608\n",
            "Current Train Reward:  -1039.4785279885834\n",
            "Current TD Error:  7.087591313117908\n",
            "\n",
            "\n",
            "Episode:  609\n",
            "Current Train Reward:  -1075.647380415941\n",
            "Current TD Error:  5.347440479313705\n",
            "\n",
            "\n",
            "Episode:  610\n",
            "Current Train Reward:  -1071.0208337716986\n",
            "Current TD Error:  5.77352856629377\n",
            "\n",
            "\n",
            "Episode:  611\n",
            "Current Train Reward:  -1077.611058074744\n",
            "Current TD Error:  5.964367504115779\n",
            "\n",
            "\n",
            "Episode:  612\n",
            "Current Train Reward:  -1117.7963486817632\n",
            "Current TD Error:  6.338603038851405\n",
            "\n",
            "\n",
            "Episode:  613\n",
            "Current Train Reward:  -1208.817795695963\n",
            "Current TD Error:  3.807813123852104\n",
            "\n",
            "\n",
            "Episode:  614\n",
            "Current Train Reward:  -1091.804126505087\n",
            "Current TD Error:  8.568482925844945\n",
            "\n",
            "\n",
            "Episode:  615\n",
            "Current Train Reward:  -1057.2202591732755\n",
            "Current TD Error:  6.383860506565405\n",
            "\n",
            "\n",
            "Episode:  616\n",
            "Current Train Reward:  -1051.5428395729584\n",
            "Current TD Error:  6.716083215488189\n",
            "\n",
            "\n",
            "Episode:  617\n",
            "Current Train Reward:  -1057.1733070060204\n",
            "Current TD Error:  10.609663961788925\n",
            "\n",
            "\n",
            "Episode:  618\n",
            "Current Train Reward:  -1107.5340654265613\n",
            "Current TD Error:  5.401427378193865\n",
            "\n",
            "\n",
            "Episode:  619\n",
            "Current Train Reward:  -1199.3480666153393\n",
            "Current TD Error:  3.519521584279345\n",
            "\n",
            "\n",
            "Episode:  620\n",
            "Current Train Reward:  -1052.4987928009436\n",
            "Current TD Error:  5.134852640781537\n",
            "\n",
            "\n",
            "Episode:  621\n",
            "Current Train Reward:  -1035.9066701632878\n",
            "Current TD Error:  7.230098206073437\n",
            "\n",
            "\n",
            "Episode:  622\n",
            "Current Train Reward:  -1041.4803673155088\n",
            "Current TD Error:  10.234141471393725\n",
            "\n",
            "\n",
            "Episode:  623\n",
            "Current Train Reward:  -1057.0163632742829\n",
            "Current TD Error:  7.430497821228372\n",
            "\n",
            "\n",
            "Episode:  624\n",
            "Current Train Reward:  -1027.9562039110342\n",
            "Current TD Error:  5.462203227902486\n",
            "\n",
            "\n",
            "Episode:  625\n",
            "Current Train Reward:  -1195.1775088379682\n",
            "Current TD Error:  5.81082871838681\n",
            "\n",
            "\n",
            "Episode:  626\n",
            "Current Train Reward:  -1115.878145094207\n",
            "Current TD Error:  3.9378545128991944\n",
            "\n",
            "\n",
            "Episode:  627\n",
            "Current Train Reward:  -1103.1311700693016\n",
            "Current TD Error:  4.096128198011439\n",
            "\n",
            "\n",
            "Episode:  628\n",
            "Current Train Reward:  -1066.4900485080038\n",
            "Current TD Error:  5.212495696279584\n",
            "\n",
            "\n",
            "Episode:  629\n",
            "Current Train Reward:  -1144.2045087922554\n",
            "Current TD Error:  3.9785648099906705\n",
            "\n",
            "\n",
            "Episode:  630\n",
            "Current Train Reward:  -1111.3089169122877\n",
            "Current TD Error:  4.619701952016775\n",
            "\n",
            "\n",
            "Episode:  631\n",
            "Current Train Reward:  -1079.0440684522855\n",
            "Current TD Error:  4.257283887061575\n",
            "\n",
            "\n",
            "Episode:  632\n",
            "Current Train Reward:  -1047.8796576911946\n",
            "Current TD Error:  4.830053311117321\n",
            "\n",
            "\n",
            "Episode:  633\n",
            "Current Train Reward:  -1098.7319951876318\n",
            "Current TD Error:  5.2690651076491895\n",
            "\n",
            "\n",
            "Episode:  634\n",
            "Current Train Reward:  -1038.8137569689582\n",
            "Current TD Error:  9.186060245704628\n",
            "\n",
            "\n",
            "Episode:  635\n",
            "Current Train Reward:  -1044.6904985918181\n",
            "Current TD Error:  6.271752078587029\n",
            "\n",
            "\n",
            "Episode:  636\n",
            "Current Train Reward:  -1162.8890742037372\n",
            "Current TD Error:  4.440070014487483\n",
            "\n",
            "\n",
            "Episode:  637\n",
            "Current Train Reward:  -1097.3756769280646\n",
            "Current TD Error:  7.340115815400409\n",
            "\n",
            "\n",
            "Episode:  638\n",
            "Current Train Reward:  -1187.2646515875933\n",
            "Current TD Error:  4.832909918883942\n",
            "\n",
            "\n",
            "Episode:  639\n",
            "Current Train Reward:  -1044.7140886815691\n",
            "Current TD Error:  10.675553748008511\n",
            "\n",
            "\n",
            "Episode:  640\n",
            "Current Train Reward:  -1026.633321502123\n",
            "Current TD Error:  5.080772632661704\n",
            "\n",
            "\n",
            "Episode:  641\n",
            "Current Train Reward:  -1106.33454412381\n",
            "Current TD Error:  4.140343986374748\n",
            "\n",
            "\n",
            "Episode:  642\n",
            "Current Train Reward:  -1031.4488676968172\n",
            "Current TD Error:  8.220078272397373\n",
            "\n",
            "\n",
            "Episode:  643\n",
            "Current Train Reward:  -1071.639357809011\n",
            "Current TD Error:  7.369290175931416\n",
            "\n",
            "\n",
            "Episode:  644\n",
            "Current Train Reward:  -1046.9751414759892\n",
            "Current TD Error:  6.311004058129152\n",
            "\n",
            "\n",
            "Episode:  645\n",
            "Current Train Reward:  -1081.8844490984761\n",
            "Current TD Error:  4.164711055157653\n",
            "\n",
            "\n",
            "Episode:  646\n",
            "Current Train Reward:  -1067.3905589985754\n",
            "Current TD Error:  6.311226003927069\n",
            "\n",
            "\n",
            "Episode:  647\n",
            "Current Train Reward:  -1118.1139818661363\n",
            "Current TD Error:  3.830042194955809\n",
            "\n",
            "\n",
            "Episode:  648\n",
            "Current Train Reward:  -1077.9875484050963\n",
            "Current TD Error:  2.91375292393458\n",
            "\n",
            "\n",
            "Episode:  649\n",
            "Current Train Reward:  -1035.652264749539\n",
            "Current TD Error:  6.284943313623976\n",
            "\n",
            "\n",
            "Episode:  650\n",
            "Current Train Reward:  -1065.6553228473658\n",
            "Current TD Error:  6.533554435917694\n",
            "\n",
            "\n",
            "Episode:  651\n",
            "Current Train Reward:  -1096.7842607322914\n",
            "Current TD Error:  4.338618122134409\n",
            "\n",
            "\n",
            "Episode:  652\n",
            "Current Train Reward:  -1156.0666651139843\n",
            "Current TD Error:  6.076589920524737\n",
            "\n",
            "\n",
            "Episode:  653\n",
            "Current Train Reward:  -1051.5443780117635\n",
            "Current TD Error:  5.8741152239556165\n",
            "\n",
            "\n",
            "Episode:  654\n",
            "Current Train Reward:  -1036.8656641892514\n",
            "Current TD Error:  6.433376750623468\n",
            "\n",
            "\n",
            "Episode:  655\n",
            "Current Train Reward:  -1095.1578087440232\n",
            "Current TD Error:  4.476398951997111\n",
            "\n",
            "\n",
            "Episode:  656\n",
            "Current Train Reward:  -1094.0511946392241\n",
            "Current TD Error:  4.96383369695259\n",
            "\n",
            "\n",
            "Episode:  657\n",
            "Current Train Reward:  -1120.0138954619845\n",
            "Current TD Error:  4.108254565360328\n",
            "\n",
            "\n",
            "Episode:  658\n",
            "Current Train Reward:  -1108.057020877403\n",
            "Current TD Error:  4.084038785944257\n",
            "\n",
            "\n",
            "Episode:  659\n",
            "Current Train Reward:  -1121.1880178823096\n",
            "Current TD Error:  7.523665518065648\n",
            "\n",
            "\n",
            "Episode:  660\n",
            "Current Train Reward:  -1048.1387916038457\n",
            "Current TD Error:  5.574941385108448\n",
            "\n",
            "\n",
            "Episode:  661\n",
            "Current Train Reward:  -1188.1103029229114\n",
            "Current TD Error:  4.902872022591287\n",
            "\n",
            "\n",
            "Episode:  662\n",
            "Current Train Reward:  -1043.3371561137694\n",
            "Current TD Error:  7.696126043553833\n",
            "\n",
            "\n",
            "Episode:  663\n",
            "Current Train Reward:  -1048.168922881013\n",
            "Current TD Error:  5.173583746018398\n",
            "\n",
            "\n",
            "Episode:  664\n",
            "Current Train Reward:  -1055.5810204654442\n",
            "Current TD Error:  7.988714077694725\n",
            "\n",
            "\n",
            "Episode:  665\n",
            "Current Train Reward:  -1161.6473933572438\n",
            "Current TD Error:  4.425712173708448\n",
            "\n",
            "\n",
            "Episode:  666\n",
            "Current Train Reward:  -1042.6568082020872\n",
            "Current TD Error:  8.711664495507444\n",
            "\n",
            "\n",
            "Episode:  667\n",
            "Current Train Reward:  -1102.4704733985795\n",
            "Current TD Error:  3.925128000286241\n",
            "\n",
            "\n",
            "Episode:  668\n",
            "Current Train Reward:  -1038.7800934282493\n",
            "Current TD Error:  7.394240474998878\n",
            "\n",
            "\n",
            "Episode:  669\n",
            "Current Train Reward:  -1077.576219675253\n",
            "Current TD Error:  5.028743016972165\n",
            "\n",
            "\n",
            "Episode:  670\n",
            "Current Train Reward:  -1087.993468384242\n",
            "Current TD Error:  5.395872590696706\n",
            "\n",
            "\n",
            "Episode:  671\n",
            "Current Train Reward:  -1141.654929893708\n",
            "Current TD Error:  3.50907380377356\n",
            "\n",
            "\n",
            "Episode:  672\n",
            "Current Train Reward:  -1074.4551923754657\n",
            "Current TD Error:  5.331052801500451\n",
            "\n",
            "\n",
            "Episode:  673\n",
            "Current Train Reward:  -1078.4457912517105\n",
            "Current TD Error:  8.03097349481954\n",
            "\n",
            "\n",
            "Episode:  674\n",
            "Current Train Reward:  -1215.9412690572453\n",
            "Current TD Error:  3.207759010220053\n",
            "\n",
            "\n",
            "Episode:  675\n",
            "Current Train Reward:  -1041.8962477534294\n",
            "Current TD Error:  5.183000040517922\n",
            "\n",
            "\n",
            "Episode:  676\n",
            "Current Train Reward:  -1047.9589774683761\n",
            "Current TD Error:  7.1246593820036\n",
            "\n",
            "\n",
            "Episode:  677\n",
            "Current Train Reward:  -1067.8700755467448\n",
            "Current TD Error:  4.277035439513205\n",
            "\n",
            "\n",
            "Episode:  678\n",
            "Current Train Reward:  -1053.072398028226\n",
            "Current TD Error:  4.6729336158252135\n",
            "\n",
            "\n",
            "Episode:  679\n",
            "Current Train Reward:  -1129.440253974652\n",
            "Current TD Error:  6.340231127084038\n",
            "\n",
            "\n",
            "Episode:  680\n",
            "Current Train Reward:  -1051.5971278601448\n",
            "Current TD Error:  10.997959676987936\n",
            "\n",
            "\n",
            "Episode:  681\n",
            "Current Train Reward:  -1161.3168472060465\n",
            "Current TD Error:  2.9892432452159214\n",
            "\n",
            "\n",
            "Episode:  682\n",
            "Current Train Reward:  -1064.5655612573564\n",
            "Current TD Error:  5.660045076013091\n",
            "\n",
            "\n",
            "Episode:  683\n",
            "Current Train Reward:  -1123.0248352750605\n",
            "Current TD Error:  4.855995362205019\n",
            "\n",
            "\n",
            "Episode:  684\n",
            "Current Train Reward:  -1048.2876230148627\n",
            "Current TD Error:  8.566695379569714\n",
            "\n",
            "\n",
            "Episode:  685\n",
            "Current Train Reward:  -1112.703209543714\n",
            "Current TD Error:  4.470815159855865\n",
            "\n",
            "\n",
            "Episode:  686\n",
            "Current Train Reward:  -1161.6281992463157\n",
            "Current TD Error:  5.165686528949709\n",
            "\n",
            "\n",
            "Episode:  687\n",
            "Current Train Reward:  -1066.3497582909647\n",
            "Current TD Error:  7.6284052063485825\n",
            "\n",
            "\n",
            "Episode:  688\n",
            "Current Train Reward:  -1139.4016581126878\n",
            "Current TD Error:  4.094861089600596\n",
            "\n",
            "\n",
            "Episode:  689\n",
            "Current Train Reward:  -1032.2512886524762\n",
            "Current TD Error:  6.872341829047084\n",
            "\n",
            "\n",
            "Episode:  690\n",
            "Current Train Reward:  -1100.0765072870827\n",
            "Current TD Error:  3.442160525317314\n",
            "\n",
            "\n",
            "Episode:  691\n",
            "Current Train Reward:  -1077.049381050703\n",
            "Current TD Error:  5.536124316816645\n",
            "\n",
            "\n",
            "Episode:  692\n",
            "Current Train Reward:  -1080.054922359671\n",
            "Current TD Error:  10.443989281626687\n",
            "\n",
            "\n",
            "Episode:  693\n",
            "Current Train Reward:  -1039.8467058896501\n",
            "Current TD Error:  6.135422064239644\n",
            "\n",
            "\n",
            "Episode:  694\n",
            "Current Train Reward:  -1041.3028093799117\n",
            "Current TD Error:  13.09969732064933\n",
            "\n",
            "\n",
            "Episode:  695\n",
            "Current Train Reward:  -1073.6803418441298\n",
            "Current TD Error:  4.672478732750272\n",
            "\n",
            "\n",
            "Episode:  696\n",
            "Current Train Reward:  -1161.3441345908639\n",
            "Current TD Error:  3.2173491806933154\n",
            "\n",
            "\n",
            "Episode:  697\n",
            "Current Train Reward:  -1076.315608078189\n",
            "Current TD Error:  4.748503700158732\n",
            "\n",
            "\n",
            "Episode:  698\n",
            "Current Train Reward:  -1096.6410281826775\n",
            "Current TD Error:  5.073184803015721\n",
            "\n",
            "\n",
            "Episode:  699\n",
            "Current Train Reward:  -1081.990129902685\n",
            "Current TD Error:  8.738588886333993\n",
            "\n",
            "\n",
            "Episode:  700\n",
            "Current Train Reward:  -1062.1506575893088\n",
            "Current TD Error:  9.181813313587224\n",
            "\n",
            "\n",
            "Current Test Reward:  -1151.2269108590438\n",
            "Episode:  701\n",
            "Current Train Reward:  -1104.5583671523507\n",
            "Current TD Error:  6.676890010691038\n",
            "\n",
            "\n",
            "Episode:  702\n",
            "Current Train Reward:  -1035.2093901389467\n",
            "Current TD Error:  5.69730468920672\n",
            "\n",
            "\n",
            "Episode:  703\n",
            "Current Train Reward:  -1065.5257071319909\n",
            "Current TD Error:  6.098165571277679\n",
            "\n",
            "\n",
            "Episode:  704\n",
            "Current Train Reward:  -1112.5021767978037\n",
            "Current TD Error:  2.894977142603187\n",
            "\n",
            "\n",
            "Episode:  705\n",
            "Current Train Reward:  -1047.4598301574138\n",
            "Current TD Error:  6.845657641958411\n",
            "\n",
            "\n",
            "Episode:  706\n",
            "Current Train Reward:  -1180.6510764800141\n",
            "Current TD Error:  5.078194234455203\n",
            "\n",
            "\n",
            "Episode:  707\n",
            "Current Train Reward:  -1117.2659695382386\n",
            "Current TD Error:  4.464656309987354\n",
            "\n",
            "\n",
            "Episode:  708\n",
            "Current Train Reward:  -1033.3080579339792\n",
            "Current TD Error:  7.976734881153011\n",
            "\n",
            "\n",
            "Episode:  709\n",
            "Current Train Reward:  -1122.789860000836\n",
            "Current TD Error:  5.909120134717419\n",
            "\n",
            "\n",
            "Episode:  710\n",
            "Current Train Reward:  -1054.8711869752028\n",
            "Current TD Error:  7.527732024543883\n",
            "\n",
            "\n",
            "Episode:  711\n",
            "Current Train Reward:  -1163.1069901489625\n",
            "Current TD Error:  3.407398066093897\n",
            "\n",
            "\n",
            "Episode:  712\n",
            "Current Train Reward:  -1078.9631139242383\n",
            "Current TD Error:  10.022143087547091\n",
            "\n",
            "\n",
            "Episode:  713\n",
            "Current Train Reward:  -1058.0045228468405\n",
            "Current TD Error:  6.158510824317964\n",
            "\n",
            "\n",
            "Episode:  714\n",
            "Current Train Reward:  -1182.085109893772\n",
            "Current TD Error:  5.812651848201538\n",
            "\n",
            "\n",
            "Episode:  715\n",
            "Current Train Reward:  -1197.8826233810955\n",
            "Current TD Error:  8.02223402378416\n",
            "\n",
            "\n",
            "Episode:  716\n",
            "Current Train Reward:  -1069.5613514611232\n",
            "Current TD Error:  10.512887252322713\n",
            "\n",
            "\n",
            "Episode:  717\n",
            "Current Train Reward:  -1049.8869962129988\n",
            "Current TD Error:  6.96897376285655\n",
            "\n",
            "\n",
            "Episode:  718\n",
            "Current Train Reward:  -1045.41480199461\n",
            "Current TD Error:  6.026545746585511\n",
            "\n",
            "\n",
            "Episode:  719\n",
            "Current Train Reward:  -1045.7295547000142\n",
            "Current TD Error:  6.383551502938929\n",
            "\n",
            "\n",
            "Episode:  720\n",
            "Current Train Reward:  -1114.8766523698323\n",
            "Current TD Error:  3.5370103178790746\n",
            "\n",
            "\n",
            "Episode:  721\n",
            "Current Train Reward:  -1037.1568207722685\n",
            "Current TD Error:  3.0754167656509934\n",
            "\n",
            "\n",
            "Episode:  722\n",
            "Current Train Reward:  -1182.8574598998005\n",
            "Current TD Error:  5.302871451273752\n",
            "\n",
            "\n",
            "Episode:  723\n",
            "Current Train Reward:  -1127.7343725379567\n",
            "Current TD Error:  5.09076253372303\n",
            "\n",
            "\n",
            "Episode:  724\n",
            "Current Train Reward:  -1040.7047577530022\n",
            "Current TD Error:  5.557435450176536\n",
            "\n",
            "\n",
            "Episode:  725\n",
            "Current Train Reward:  -1051.731292011324\n",
            "Current TD Error:  5.271354517553261\n",
            "\n",
            "\n",
            "Episode:  726\n",
            "Current Train Reward:  -1065.513029862055\n",
            "Current TD Error:  6.766937115740192\n",
            "\n",
            "\n",
            "Episode:  727\n",
            "Current Train Reward:  -1148.7647131128278\n",
            "Current TD Error:  4.127970480863854\n",
            "\n",
            "\n",
            "Episode:  728\n",
            "Current Train Reward:  -1041.490672715064\n",
            "Current TD Error:  8.509954614290013\n",
            "\n",
            "\n",
            "Episode:  729\n",
            "Current Train Reward:  -1100.6246449588978\n",
            "Current TD Error:  4.081148823769601\n",
            "\n",
            "\n",
            "Episode:  730\n",
            "Current Train Reward:  -1109.7745928339566\n",
            "Current TD Error:  4.084488075334988\n",
            "\n",
            "\n",
            "Episode:  731\n",
            "Current Train Reward:  -1082.4902029223078\n",
            "Current TD Error:  5.804001082757443\n",
            "\n",
            "\n",
            "Episode:  732\n",
            "Current Train Reward:  -1107.5485555724013\n",
            "Current TD Error:  3.5887410184512314\n",
            "\n",
            "\n",
            "Episode:  733\n",
            "Current Train Reward:  -1140.1151547549507\n",
            "Current TD Error:  4.35358468691431\n",
            "\n",
            "\n",
            "Episode:  734\n",
            "Current Train Reward:  -1092.916723229565\n",
            "Current TD Error:  5.992141351542889\n",
            "\n",
            "\n",
            "Episode:  735\n",
            "Current Train Reward:  -1075.0556636683584\n",
            "Current TD Error:  9.27937846530552\n",
            "\n",
            "\n",
            "Episode:  736\n",
            "Current Train Reward:  -1075.5311437947582\n",
            "Current TD Error:  6.083454315696134\n",
            "\n",
            "\n",
            "Episode:  737\n",
            "Current Train Reward:  -1070.1484791292737\n",
            "Current TD Error:  6.426023057985472\n",
            "\n",
            "\n",
            "Episode:  738\n",
            "Current Train Reward:  -1045.677346501238\n",
            "Current TD Error:  8.056966251552875\n",
            "\n",
            "\n",
            "Episode:  739\n",
            "Current Train Reward:  -1145.9093309610294\n",
            "Current TD Error:  5.951703955297378\n",
            "\n",
            "\n",
            "Episode:  740\n",
            "Current Train Reward:  -1142.1402441046018\n",
            "Current TD Error:  3.979447098363677\n",
            "\n",
            "\n",
            "Episode:  741\n",
            "Current Train Reward:  -1035.677274890939\n",
            "Current TD Error:  6.213446234779999\n",
            "\n",
            "\n",
            "Episode:  742\n",
            "Current Train Reward:  -1096.0580564476118\n",
            "Current TD Error:  7.650671413994098\n",
            "\n",
            "\n",
            "Episode:  743\n",
            "Current Train Reward:  -1116.5664622873362\n",
            "Current TD Error:  4.147714927133356\n",
            "\n",
            "\n",
            "Episode:  744\n",
            "Current Train Reward:  -1130.8439431029517\n",
            "Current TD Error:  6.729446518888346\n",
            "\n",
            "\n",
            "Episode:  745\n",
            "Current Train Reward:  -1144.0248185778114\n",
            "Current TD Error:  5.754044482012104\n",
            "\n",
            "\n",
            "Episode:  746\n",
            "Current Train Reward:  -1053.444581103613\n",
            "Current TD Error:  9.415533320098453\n",
            "\n",
            "\n",
            "Episode:  747\n",
            "Current Train Reward:  -1133.7065935269743\n",
            "Current TD Error:  5.9334200848153325\n",
            "\n",
            "\n",
            "Episode:  748\n",
            "Current Train Reward:  -1077.6724220693802\n",
            "Current TD Error:  5.306474242163362\n",
            "\n",
            "\n",
            "Episode:  749\n",
            "Current Train Reward:  -1036.372276260075\n",
            "Current TD Error:  7.034300286689377\n",
            "\n",
            "\n",
            "Episode:  750\n",
            "Current Train Reward:  -1115.8326206924423\n",
            "Current TD Error:  4.250586627217024\n",
            "\n",
            "\n",
            "Episode:  751\n",
            "Current Train Reward:  -1065.415161211078\n",
            "Current TD Error:  10.860124637687758\n",
            "\n",
            "\n",
            "Episode:  752\n",
            "Current Train Reward:  -1058.2904980450585\n",
            "Current TD Error:  5.396166064239422\n",
            "\n",
            "\n",
            "Episode:  753\n",
            "Current Train Reward:  -1059.853855107305\n",
            "Current TD Error:  5.830860638347016\n",
            "\n",
            "\n",
            "Episode:  754\n",
            "Current Train Reward:  -1147.5500142656547\n",
            "Current TD Error:  4.488978009417525\n",
            "\n",
            "\n",
            "Episode:  755\n",
            "Current Train Reward:  -1115.1428894038336\n",
            "Current TD Error:  3.956210907334473\n",
            "\n",
            "\n",
            "Episode:  756\n",
            "Current Train Reward:  -1119.676570914693\n",
            "Current TD Error:  6.148722685327859\n",
            "\n",
            "\n",
            "Episode:  757\n",
            "Current Train Reward:  -1103.5506979407876\n",
            "Current TD Error:  7.277948210258949\n",
            "\n",
            "\n",
            "Episode:  758\n",
            "Current Train Reward:  -1108.9839287678196\n",
            "Current TD Error:  4.768619784877384\n",
            "\n",
            "\n",
            "Episode:  759\n",
            "Current Train Reward:  -1034.7499211898971\n",
            "Current TD Error:  7.2409727231481975\n",
            "\n",
            "\n",
            "Episode:  760\n",
            "Current Train Reward:  -1037.3353948579258\n",
            "Current TD Error:  6.563482891891139\n",
            "\n",
            "\n",
            "Episode:  761\n",
            "Current Train Reward:  -1188.2994883663494\n",
            "Current TD Error:  9.534426595984877\n",
            "\n",
            "\n",
            "Episode:  762\n",
            "Current Train Reward:  -1050.464044250461\n",
            "Current TD Error:  9.076310488201067\n",
            "\n",
            "\n",
            "Episode:  763\n",
            "Current Train Reward:  -1121.752613499081\n",
            "Current TD Error:  4.862174155178103\n",
            "\n",
            "\n",
            "Episode:  764\n",
            "Current Train Reward:  -1126.0194569657042\n",
            "Current TD Error:  4.112957987706812\n",
            "\n",
            "\n",
            "Episode:  765\n",
            "Current Train Reward:  -1058.6506516937807\n",
            "Current TD Error:  11.485378489179126\n",
            "\n",
            "\n",
            "Episode:  766\n",
            "Current Train Reward:  -1158.8703270308756\n",
            "Current TD Error:  5.646581329440927\n",
            "\n",
            "\n",
            "Episode:  767\n",
            "Current Train Reward:  -1033.6145426107985\n",
            "Current TD Error:  6.639360035328883\n",
            "\n",
            "\n",
            "Episode:  768\n",
            "Current Train Reward:  -1090.1650254144183\n",
            "Current TD Error:  7.585794369708701\n",
            "\n",
            "\n",
            "Episode:  769\n",
            "Current Train Reward:  -1099.1328557854067\n",
            "Current TD Error:  7.206543257495546\n",
            "\n",
            "\n",
            "Episode:  770\n",
            "Current Train Reward:  -1085.2339900175878\n",
            "Current TD Error:  8.141746274581223\n",
            "\n",
            "\n",
            "Episode:  771\n",
            "Current Train Reward:  -1034.7356448093299\n",
            "Current TD Error:  7.213403045684799\n",
            "\n",
            "\n",
            "Episode:  772\n",
            "Current Train Reward:  -1144.546431290313\n",
            "Current TD Error:  4.092707460352038\n",
            "\n",
            "\n",
            "Episode:  773\n",
            "Current Train Reward:  -1044.0374804616554\n",
            "Current TD Error:  7.823576792385156\n",
            "\n",
            "\n",
            "Episode:  774\n",
            "Current Train Reward:  -1066.3082426932644\n",
            "Current TD Error:  6.780281353469439\n",
            "\n",
            "\n",
            "Episode:  775\n",
            "Current Train Reward:  -1174.980049831997\n",
            "Current TD Error:  5.513993633388426\n",
            "\n",
            "\n",
            "Episode:  776\n",
            "Current Train Reward:  -1049.05536253081\n",
            "Current TD Error:  8.700791364917169\n",
            "\n",
            "\n",
            "Episode:  777\n",
            "Current Train Reward:  -1176.4690579475912\n",
            "Current TD Error:  6.312126165977742\n",
            "\n",
            "\n",
            "Episode:  778\n",
            "Current Train Reward:  -1056.798745257655\n",
            "Current TD Error:  3.9868547122322644\n",
            "\n",
            "\n",
            "Episode:  779\n",
            "Current Train Reward:  -1130.4763730238521\n",
            "Current TD Error:  5.346880283567496\n",
            "\n",
            "\n",
            "Episode:  780\n",
            "Current Train Reward:  -1085.8952774481247\n",
            "Current TD Error:  5.834432424050986\n",
            "\n",
            "\n",
            "Episode:  781\n",
            "Current Train Reward:  -1120.3592958530896\n",
            "Current TD Error:  7.078729122468693\n",
            "\n",
            "\n",
            "Episode:  782\n",
            "Current Train Reward:  -1177.4737339533372\n",
            "Current TD Error:  5.129129401315542\n",
            "\n",
            "\n",
            "Episode:  783\n",
            "Current Train Reward:  -1040.316557504778\n",
            "Current TD Error:  8.290934563269635\n",
            "\n",
            "\n",
            "Episode:  784\n",
            "Current Train Reward:  -1098.5938517266013\n",
            "Current TD Error:  6.236178042564897\n",
            "\n",
            "\n",
            "Episode:  785\n",
            "Current Train Reward:  -1040.9003607776544\n",
            "Current TD Error:  4.770500285310665\n",
            "\n",
            "\n",
            "Episode:  786\n",
            "Current Train Reward:  -1076.903956244949\n",
            "Current TD Error:  7.468347329110825\n",
            "\n",
            "\n",
            "Episode:  787\n",
            "Current Train Reward:  -1042.413581779357\n",
            "Current TD Error:  12.9704876230755\n",
            "\n",
            "\n",
            "Episode:  788\n",
            "Current Train Reward:  -1154.4183977395357\n",
            "Current TD Error:  4.500400679829721\n",
            "\n",
            "\n",
            "Episode:  789\n",
            "Current Train Reward:  -1143.3007185012284\n",
            "Current TD Error:  6.296753968362261\n",
            "\n",
            "\n",
            "Episode:  790\n",
            "Current Train Reward:  -1193.1210546896896\n",
            "Current TD Error:  6.212632220711432\n",
            "\n",
            "\n",
            "Episode:  791\n",
            "Current Train Reward:  -1056.5532214915227\n",
            "Current TD Error:  4.972524724838078\n",
            "\n",
            "\n",
            "Episode:  792\n",
            "Current Train Reward:  -1185.9369075550117\n",
            "Current TD Error:  5.600347534171834\n",
            "\n",
            "\n",
            "Episode:  793\n",
            "Current Train Reward:  -1190.877578159988\n",
            "Current TD Error:  7.918953418663319\n",
            "\n",
            "\n",
            "Episode:  794\n",
            "Current Train Reward:  -1080.1743011258175\n",
            "Current TD Error:  4.642826615340811\n",
            "\n",
            "\n",
            "Episode:  795\n",
            "Current Train Reward:  -1118.3986094401428\n",
            "Current TD Error:  4.283461339159465\n",
            "\n",
            "\n",
            "Episode:  796\n",
            "Current Train Reward:  -1184.7663051678396\n",
            "Current TD Error:  8.732202650910896\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-98be593047ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-98be593047ae>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mMODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN_Agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRLTetherAviary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgui\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# MODEL = DQN_Agent(RLCrazyFlieAviary(gui=False, record=False))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-98be593047ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m                                 \u001b[0mepisode_td\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-98be593047ae>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m                         \u001b[0mobs_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                         \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                         \u001b[0mtarget_q_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1577\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \"\"\"\n\u001b[1;32m   1694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4043\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4046\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4047\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m   2938\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2939\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3080\u001b[0m         \u001b[0;31m# places (like Keras) where the FuncGraph lives longer than the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m         \u001b[0;31m# ConcreteFunction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m         shared_func_graph=False)\n\u001b[0m\u001b[1;32m   3083\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[0;31m# FuncGraph directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0;32m-> 1542\u001b[0;31m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0m\u001b[1;32m   1543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_higher_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[1;32m    604\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[1;32m    605\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n\u001b[0m\u001b[1;32m    607\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[1;32m    460\u001b[0m       \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FunctionToFunctionDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m       \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mfunction_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m     \u001b[0mfunction_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIp2kyXVWGih"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}