{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_Action_Array.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiySGCfV2yMwM8+tMUniiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukeSchmitt96/gym-pybullet-drones/blob/master/tether_sim/DQN_Action_Array.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjhCsjmOPVUV"
      },
      "source": [
        "Runtime>Change runtime type>Hardware accelerator: Change it to 'GPU'>SAVE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYV4bFKuP3c8"
      },
      "source": [
        "Google Colab automatically closes after 12 hours. Save files before that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg1kmS5VP-YE"
      },
      "source": [
        "Use autoclicker: https://sourceforge.net/projects/orphamielautoclicker/files/AutoClicker.exe/download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmI6TI83PWaN"
      },
      "source": [
        "!git clone https://github.com/LukeSchmitt96/gym-pybullet-drones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6U2OidYOFVv"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOgU0XifOLuJ"
      },
      "source": [
        "pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BRce4U9ONML"
      },
      "source": [
        "pip install stable-baselines3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URW5PoIiOOwu"
      },
      "source": [
        "pip install 'ray[rllib]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52o0Y1MjNpwU"
      },
      "source": [
        "cd gym-pybullet-drones/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrlGJLNcN9rg"
      },
      "source": [
        "pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4XOplT3tNjDU",
        "outputId": "ffa15128-e970-4087-ef69-541e99637b59"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import keras, tensorflow as tf, numpy as np, gym, sys, copy, argparse\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import model_from_json\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from gym_pybullet_drones.envs.RLTetherAviary import RLTetherAviary\n",
        "# from gym_pybullet_drones.envs.RLCrazyFlieAviary import RLCrazyFlieAviary\n",
        "\n",
        "class DQN():\n",
        "\n",
        "\tdef __init__(self, environment):\n",
        "\t\t\n",
        "\t\tself.environment = environment\n",
        "\t\tself.num_actions = self.environment.action_space.shape[0] # Pitch forward, Pitch backward, Roll left, Roll right, Hover\n",
        "\t\tself.state_size = self.environment.reset().shape[0]\n",
        "\t\tself.model = Sequential()\n",
        "\t\tself.model.add(Dense(128, input_dim=self.state_size, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\t#self.model.add(Dense(64, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\tself.model.add(Dense(64, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\tself.model.add(Dense(32, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\tself.model.add(Dense(self.num_actions))\n",
        "\t\tself.model.compile(loss = 'mse', optimizer = Adam(lr = 0.001)) # Define loss and learning rate\n",
        "\n",
        "\tdef save_model_weights(self):\n",
        "\t\t\n",
        "\t\tmodel_json = self.model.to_json()\n",
        "\t\twith open(\"model.json\", \"w\") as json_file:\n",
        "\t\t\tjson_file.write(model_json)\n",
        "\t\tself.model.save_weights(\"model.h5\")\n",
        "\n",
        "\tdef load_model(self, model_file):\n",
        "\t\t\n",
        "\t\tjson_file = open('model.json', 'r')\n",
        "\t\tloaded_model_json = json_file.read()\n",
        "\t\tjson_file.close()\n",
        "\t\tself.model = model_from_json(loaded_model_json)\n",
        "\n",
        "\tdef load_model_weights(self,weight_file):\n",
        "\t\t\n",
        "\t\tself.model.load_weights(\"model.h5\")\n",
        "\n",
        "\n",
        "class Replay_Memory():\n",
        "\n",
        "\tdef __init__(self, memory_size = 50000): # Define replay memory size\n",
        "\t\t\n",
        "\t \tself.memory = collections.deque(maxlen = memory_size)\n",
        "\n",
        "\tdef sample_batch(self, batch_size = 32): # Define batch size\n",
        "\t\t\n",
        "\t\tif len(self.memory) < batch_size:\n",
        "\t\t\treturn None\n",
        "\t\telse:\n",
        "\t\t\treturn random.sample(self.memory, batch_size)\n",
        "\n",
        "\tdef append(self, transition):\n",
        "\t\t\n",
        "\t\tself.memory.append(transition)\n",
        "\n",
        "class DQN_Agent():\n",
        "\n",
        "\tdef __init__(self, environment_name):\n",
        "\n",
        "\t\t# self.environment_name = environment_name\n",
        "\t\tself.env = environment_name # gym.envs.make(self.environment_name)\n",
        "\t\tself.train_network = DQN(self.env)\n",
        "\t\tself.target_network = DQN(self.env)\n",
        "\t\tself.target_network.model.set_weights(self.train_network.model.get_weights())\n",
        "\t\tself.memory = Replay_Memory()\n",
        "\t\tself.num_episodes = 10000 # Define number of episodes\n",
        "\t\tself.gamma = 0.99 # Define discount rate\n",
        "\t\tself.eps_limit = 0.5 # Define Epsilon\n",
        "\t\tself.target_update = 5 # Define target net update frequency\n",
        "\n",
        "\tdef index_to_array(self, index):\n",
        "\n",
        "\t\ttether_force = 0\n",
        "\t\tmin = 0.1\n",
        "\t\tmax = 0.2\n",
        "\t\thov = -0.0001\n",
        "\n",
        "\t\tif index == 0:\n",
        "\t\t\taction_array = np.array([min, min, max, max, tether_force]) # Rotor 3 and Rotor 4, X-axis\n",
        "\t\telif index == 1:\n",
        "\t\t\taction_array = np.array([max, max, min, min, tether_force]) # Rotor 1 and Rotor 2, X-axis\n",
        "\t\telif index == 2:\n",
        "\t\t\taction_array = np.array([max, min, max, min, tether_force]) # Rotor 1 and Rotor 3, Y-axis\n",
        "\t\telif index == 3:\n",
        "\t\t\taction_array = np.array([min, max, min, max, tether_force]) # Rotor 2 and Rotor 4, Y-axis\n",
        "\t\telif index == 4:\n",
        "\t\t\taction_array = np.array([hov, hov, hov, hov, 0]) # Hover\n",
        "\t\t\n",
        "\t\treturn action_array\n",
        "\n",
        "\tdef epsilon_greedy_policy(self, q_values):\n",
        "\t\t\n",
        "\t\teps = np.random.random(1)[0]\n",
        "\t\tif eps < self.eps_limit:\n",
        "\t\t\taction_index = np.random.randint(0, self.train_network.num_actions)\n",
        "\t\n",
        "\t\telse:\n",
        "\t\t\taction_index = np.argmax(q_values)\n",
        "\t\t\n",
        "\t\taction = self.index_to_array(action_index)\n",
        "\n",
        "\t\tcurrent_q = q_values[action_index]\n",
        "\t\t\n",
        "\t\treturn action_index, action, current_q\n",
        "\n",
        "\tdef greedy_policy(self, q_values):\n",
        "\t\t\n",
        "\t\taction = np.argmax(q_values)\n",
        "\n",
        "\t\treturn action\n",
        "\t\n",
        "\tdef update_model(self):\n",
        "\t\n",
        "\t\tstates = []\n",
        "\t\tobs_states = []\n",
        "\t\tbatch = self.memory.sample_batch()\n",
        "\n",
        "\t\tif batch == None:\n",
        "\t\t\treturn\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tfor tup in batch:\n",
        "\t\t\t\tstate, action_index, reward, obs, done = tup\n",
        "\t\t\t\tstates.append(state)\n",
        "\t\t\t\tobs_states.append(obs)\n",
        "\t\t\t\n",
        "\t\t\tstates = np.asarray(states)\n",
        "\t\t\tobs_states = np.asarray(obs_states)\n",
        "\n",
        "\t\t\ttarget_q = (self.train_network.model.predict(states))\n",
        "\t\t\ttarget_q_obs = (self.target_network.model.predict(obs_states))\n",
        "\t\t\t\t\n",
        "\t\t\ti = 0\t\n",
        "\t\t\tfor tup in batch:\n",
        "\t\t\t\tstate, action_index, reward, obs, done = tup\n",
        "\t\t\t\tif done:\n",
        "\t\t\t\t\ttarget_q[i][action_index] = reward\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tq_s = max(target_q_obs[i])\n",
        "\t\t\t\t\ttarget_q[i][action_index] = reward + q_s * self.gamma\n",
        "\t\t\t\ti += 1\n",
        "\n",
        "\t\t\tself.train_network.model.train_on_batch(states, target_q) ##### Backprop qvalues\n",
        "\n",
        "\tdef train(self):\n",
        "\t\t\n",
        "\t\tself.test_list = []\n",
        "\t\tself.train_td = []\n",
        "\t\tepisode = 0\n",
        "\t\twhile episode < self.num_episodes:\n",
        "\t\t\tdone = False\n",
        "\t\t\tstate = self.env.reset()\n",
        "\t\t\tepisode_td = []\n",
        "\t\t\tepisode_reward = 0\n",
        "\t\t\twhile not done:\n",
        "\t\t\t\tq_values = (self.train_network.model.predict(state[None, :]))[0]\n",
        "\t\t\t\taction_index, action, q = self.epsilon_greedy_policy(q_values)\n",
        "\t\t\t\tobs, reward, done,  _ = self.env.step(action)\n",
        "\t\t\t\tepisode_reward += reward\n",
        "\t\t\t\t\n",
        "\t\t\t\ttd_step = abs(reward + self.gamma * np.max((self.target_network.model.predict(obs[None, :]))) - q)\n",
        "\t\t\t\tepisode_td.append(td_step)\n",
        "\t\t\t\tself.memory.append((state, action_index, reward, obs, done))\n",
        "\t\t\t\tself.update_model()\n",
        "\t\t\t\tstate = obs\n",
        "\t\t\t\n",
        "\t\t\tself.train_td.append(np.mean(np.array(episode_td)))\n",
        "\n",
        "\t\t\tprint('Episode: ', episode)\n",
        "\t\t\tprint('Current Train Reward: ', episode_reward)\n",
        "\t\t\tprint('Current TD Error: ', self.train_td[-1])\n",
        "\t\t\tprint('\\n')\n",
        "\n",
        "\t\t\tif episode % self.target_update == 0:\n",
        "\t\t\t\tself.target_network.model.set_weights(self.train_network.model.get_weights())\n",
        "\t\t\t\n",
        "\t\t\tif episode > 50:\n",
        "\t\t\t\tself.eps_limit = max(self.eps_limit - 0.005, 0.01) # Define exploration decay rate\n",
        "\t\t\t\n",
        "\t\t\tif episode % 100 == 0:\n",
        "\t\t\t\tself.train_network.save_model_weights()\n",
        "\t\t\t\n",
        "\t\t\tif episode % 100 == 0:\n",
        "\t\t\t\tcurrent_test_reward = self.test(20)\n",
        "\t\t\t\tself.test_list.append(current_test_reward)\n",
        "\t\t\t\tprint('Current Test Reward: ', current_test_reward)\n",
        "\t\t\t\tnp.savetxt('td_error.txt', self.train_td)\n",
        "\t\t\t\tnp.savetxt('test_list.txt', self.test_list)\n",
        "\t\t\t\t\n",
        "\t\t\t\tif current_test_reward >= 10:\n",
        "\t\t\t\t\tprint('Problem solved!')\n",
        "\t\t\t\t\tself.train_network.save_model_weights()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tself.num_episodes = episode + 2\n",
        "\n",
        "\t\t\tepisode += 1\n",
        "\t\t\n",
        "\t\tself.test_reward = self.test_list\n",
        "\t\t\n",
        "\t\tnp.savetxt('td.txt', self.train_td)\n",
        "\t\tnp.savetxt('reward.txt', self.test_reward)\n",
        "\t\tplt.figure(figsize = (12, 8))\n",
        "\t\tplt.plot(np.arange(len(self.test_reward)) * 100, np.ones(len(self.test_reward)) * 200, label = 'Target reward')\n",
        "\t\tplt.plot(np.arange(len(self.test_reward)) * 100, self.test_reward, label = 'Mean reward')\n",
        "\t\tplt.grid()\n",
        "\t\tplt.xlabel('Number of episodes', fontsize = 18)\n",
        "\t\tplt.ylabel('Cumulative reward', fontsize = 18)\n",
        "\t\tplt.legend(fontsize = 18)\n",
        "\t\tplt.show()\n",
        "\t\t\n",
        "\t\tplt.figure(figsize = (12, 8))\n",
        "\t\tplt.plot(np.arange(len(self.train_td)) * 100, self.train_td, label = 'Train TD Error')\n",
        "\t\tplt.grid()\n",
        "\t\tplt.xlabel('Number of Training Episodes', fontsize = 18)\n",
        "\t\tplt.ylabel('Average TD Error', fontsize = 18)\n",
        "\t\tplt.legend(fontsize = 18)\n",
        "\t\tplt.show()\n",
        "\n",
        "\tdef test(self, episodes):\n",
        "\t\t\n",
        "\t\ttest_reward = []\n",
        "\t\tfor i in range(episodes):\n",
        "\t\t\tdone = False\n",
        "\t\t\tstate = self.env.reset()\n",
        "\t\t\tepisode_reward = 0\n",
        "\t\t\twhile not done:\n",
        "\t\t\t\taction_index = np.argmax(self.target_network.model.predict(state[None, :]))\n",
        "\t\t\t\taction = self.index_to_array(action_index)\n",
        "\t\t\t\tobs, reward, done, _ = self.env.step(action)\n",
        "\t\t\t\tepisode_reward += reward\n",
        "\t\t\t\tstate = obs\n",
        "\t\t\ttest_reward.append(episode_reward)\n",
        "\n",
        "\t\treturn np.mean(test_reward)\n",
        "\n",
        "def main():\n",
        "\n",
        "\tMODEL = DQN_Agent(RLTetherAviary(gui = 1, record = 0))\n",
        "\t# MODEL = DQN_Agent(RLCrazyFlieAviary(gui=False, record=False))\n",
        "\tMODEL.train()\n",
        "\t\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
            "[INFO] m 0.027000, L 0.039700,\n",
            "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
            "[INFO] kf 0.000000, km 0.000000,\n",
            "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
            "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
            "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
            "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode:  0\n",
            "Current Train Reward:  -1067.4089681184469\n",
            "Current TD Error:  11.352099458897245\n",
            "\n",
            "\n",
            "Current Test Reward:  -646.38571320746\n",
            "Episode:  1\n",
            "Current Train Reward:  -1076.1957618237582\n",
            "Current TD Error:  12.306867518475366\n",
            "\n",
            "\n",
            "Episode:  2\n",
            "Current Train Reward:  -1061.017228887755\n",
            "Current TD Error:  15.397074792627278\n",
            "\n",
            "\n",
            "Episode:  3\n",
            "Current Train Reward:  -1093.3030452358141\n",
            "Current TD Error:  12.947092803746344\n",
            "\n",
            "\n",
            "Episode:  4\n",
            "Current Train Reward:  -1053.305987814216\n",
            "Current TD Error:  13.479120927647045\n",
            "\n",
            "\n",
            "Episode:  5\n",
            "Current Train Reward:  -1075.0290280509728\n",
            "Current TD Error:  14.640409445004533\n",
            "\n",
            "\n",
            "Episode:  6\n",
            "Current Train Reward:  -1096.8166831591402\n",
            "Current TD Error:  13.89144440284921\n",
            "\n",
            "\n",
            "Episode:  7\n",
            "Current Train Reward:  -1057.2788377955753\n",
            "Current TD Error:  17.650090349193295\n",
            "\n",
            "\n",
            "Episode:  8\n",
            "Current Train Reward:  -1083.3181246337872\n",
            "Current TD Error:  15.252228571182847\n",
            "\n",
            "\n",
            "Episode:  9\n",
            "Current Train Reward:  -1057.6244368979299\n",
            "Current TD Error:  17.136814011714552\n",
            "\n",
            "\n",
            "Episode:  10\n",
            "Current Train Reward:  -1082.7036368439522\n",
            "Current TD Error:  16.52230368955175\n",
            "\n",
            "\n",
            "Episode:  11\n",
            "Current Train Reward:  -1089.06197193348\n",
            "Current TD Error:  18.265190487785734\n",
            "\n",
            "\n",
            "Episode:  12\n",
            "Current Train Reward:  -1066.376881054987\n",
            "Current TD Error:  16.150563531656914\n",
            "\n",
            "\n",
            "Episode:  13\n",
            "Current Train Reward:  -1093.7707662030975\n",
            "Current TD Error:  19.586239341170984\n",
            "\n",
            "\n",
            "Episode:  14\n",
            "Current Train Reward:  -1061.3712088727978\n",
            "Current TD Error:  15.583153065531663\n",
            "\n",
            "\n",
            "Episode:  15\n",
            "Current Train Reward:  -1059.6658692575907\n",
            "Current TD Error:  20.651557698243092\n",
            "\n",
            "\n",
            "Episode:  16\n",
            "Current Train Reward:  -1086.0683876720414\n",
            "Current TD Error:  21.677006525104325\n",
            "\n",
            "\n",
            "Episode:  17\n",
            "Current Train Reward:  -1080.837671326367\n",
            "Current TD Error:  19.40570798513464\n",
            "\n",
            "\n",
            "Episode:  18\n",
            "Current Train Reward:  -1059.5304580716186\n",
            "Current TD Error:  26.578423087353837\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'Identity' has no attr named '_read_only_resource_inputs'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c0661b93d1d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-c0661b93d1d1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mMODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN_Agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRLTetherAviary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgui\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# MODEL = DQN_Agent(RLCrazyFlieAviary(gui=False, record=False))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c0661b93d1d1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m                                 \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                                 \u001b[0mtd_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                                 \u001b[0mepisode_td\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1577\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1725\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m     \"\"\"\n\u001b[0;32m-> 1727\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4121\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4122\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4123\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4125\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m   2938\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2939\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         if x is not None)\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    396\u001b[0m       \u001b[0;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m       \u001b[0;31m# and last_write_to_resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m   \u001b[0;34m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m   \u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_read_write_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m   \u001b[0msaturated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msaturated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/auto_control_deps_utils.py\u001b[0m in \u001b[0;36mget_read_write_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mread_only_input_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAD_ONLY_RESOURCE_INPUTS_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2484\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Zi5g2VOnJb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}