{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_Action_Array.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPH5c5aUp68S+q1nIGhDTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukeSchmitt96/gym-pybullet-drones/blob/master/tether_sim/DQN_Action_Array.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjhCsjmOPVUV"
      },
      "source": [
        "Runtime>Change runtime type>Hardware accelerator: Change it to 'GPU'>SAVE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYV4bFKuP3c8"
      },
      "source": [
        "Google Colab automatically closes after 12 hours. Save files before that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg1kmS5VP-YE"
      },
      "source": [
        "Use autoclicker: https://sourceforge.net/projects/orphamielautoclicker/files/AutoClicker.exe/download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmI6TI83PWaN"
      },
      "source": [
        "!git clone https://github.com/LukeSchmitt96/gym-pybullet-drones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6U2OidYOFVv"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOgU0XifOLuJ"
      },
      "source": [
        "pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BRce4U9ONML"
      },
      "source": [
        "pip install stable-baselines3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URW5PoIiOOwu"
      },
      "source": [
        "pip install 'ray[rllib]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52o0Y1MjNpwU"
      },
      "source": [
        "cd gym-pybullet-drones/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrlGJLNcN9rg"
      },
      "source": [
        "pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XOplT3tNjDU"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import keras, tensorflow as tf, numpy as np, gym, sys, copy, argparse\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import model_from_json\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from gym_pybullet_drones.envs.RLTetherAviary import RLTetherAviary\n",
        "# from gym_pybullet_drones.envs.RLCrazyFlieAviary import RLCrazyFlieAviary\n",
        "\n",
        "class DQN():\n",
        "\n",
        "\tdef __init__(self, environment):\n",
        "\t\t\n",
        "\t\tself.environment = environment\n",
        "\t\tself.num_actions = self.environment.action_space.shape[0] # Pitch forward, Pitch backward, Roll left, Roll right, Hover\n",
        "\t\tself.state_size = self.environment.reset().shape[0]\n",
        "\t\tself.model = Sequential()\n",
        "\t\tself.model.add(Dense(128, input_dim=self.state_size, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\t#self.model.add(Dense(64, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\tself.model.add(Dense(64, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\tself.model.add(Dense(32, activation = 'relu')) # Define number of layers, neurons and activation\n",
        "\t\tself.model.add(Dense(self.num_actions))\n",
        "\t\tself.model.compile(loss = 'mse', optimizer = Adam(lr = 0.001)) # Define loss and learning rate\n",
        "\n",
        "\tdef save_model_weights(self):\n",
        "\t\t\n",
        "\t\tmodel_json = self.model.to_json()\n",
        "\t\twith open(\"model.json\", \"w\") as json_file:\n",
        "\t\t\tjson_file.write(model_json)\n",
        "\t\tself.model.save_weights(\"model.h5\")\n",
        "\n",
        "\tdef load_model(self, model_file):\n",
        "\t\t\n",
        "\t\tjson_file = open('model.json', 'r')\n",
        "\t\tloaded_model_json = json_file.read()\n",
        "\t\tjson_file.close()\n",
        "\t\tself.model = model_from_json(loaded_model_json)\n",
        "\n",
        "\tdef load_model_weights(self,weight_file):\n",
        "\t\t\n",
        "\t\tself.model.load_weights(\"model.h5\")\n",
        "\n",
        "\n",
        "class Replay_Memory():\n",
        "\n",
        "\tdef __init__(self, memory_size = 50000): # Define replay memory size\n",
        "\t\t\n",
        "\t \tself.memory = collections.deque(maxlen = memory_size)\n",
        "\n",
        "\tdef sample_batch(self, batch_size = 32): # Define batch size\n",
        "\t\t\n",
        "\t\tif len(self.memory) < batch_size:\n",
        "\t\t\treturn None\n",
        "\t\telse:\n",
        "\t\t\treturn random.sample(self.memory, batch_size)\n",
        "\n",
        "\tdef append(self, transition):\n",
        "\t\t\n",
        "\t\tself.memory.append(transition)\n",
        "\n",
        "class DQN_Agent():\n",
        "\n",
        "\tdef __init__(self, environment_name):\n",
        "\n",
        "\t\t# self.environment_name = environment_name\n",
        "\t\tself.env = environment_name # gym.envs.make(self.environment_name)\n",
        "\t\tself.train_network = DQN(self.env)\n",
        "\t\tself.target_network = DQN(self.env)\n",
        "\t\tself.target_network.model.set_weights(self.train_network.model.get_weights())\n",
        "\t\tself.memory = Replay_Memory()\n",
        "\t\tself.num_episodes = 10000 # Define number of episodes\n",
        "\t\tself.gamma = 0.99 # Define discount rate\n",
        "\t\tself.eps_limit = 0.5 # Define Epsilon\n",
        "\t\tself.target_update = 5 # Define target net update frequency\n",
        "\n",
        "\tdef index_to_array(self, index):\n",
        "\n",
        "\t\ttether_force = 0\n",
        "\t\tmin = 0.1\n",
        "\t\tmax = 0.2\n",
        "\t\thov = -0.0001\n",
        "\n",
        "\t\tif index == 0:\n",
        "\t\t\taction_array = np.array([min, min, max, max, tether_force]) # Rotor 3 and Rotor 4, X-axis\n",
        "\t\telif index == 1:\n",
        "\t\t\taction_array = np.array([max, max, min, min, tether_force]) # Rotor 1 and Rotor 2, X-axis\n",
        "\t\telif index == 2:\n",
        "\t\t\taction_array = np.array([max, min, max, min, tether_force]) # Rotor 1 and Rotor 3, Y-axis\n",
        "\t\telif index == 3:\n",
        "\t\t\taction_array = np.array([min, max, min, max, tether_force]) # Rotor 2 and Rotor 4, Y-axis\n",
        "\t\telif index == 4:\n",
        "\t\t\taction_array = np.array([hov, hov, hov, hov, 0]) # Hover\n",
        "\t\t\n",
        "\t\treturn action_array\n",
        "\n",
        "\tdef epsilon_greedy_policy(self, q_values):\n",
        "\t\t\n",
        "\t\teps = np.random.random(1)[0]\n",
        "\t\tif eps < self.eps_limit:\n",
        "\t\t\taction_index = np.random.randint(0, self.train_network.num_actions)\n",
        "\t\n",
        "\t\telse:\n",
        "\t\t\taction_index = np.argmax(q_values)\n",
        "\t\t\n",
        "\t\taction = self.index_to_array(action_index)\n",
        "\n",
        "\t\tcurrent_q = q_values[action_index]\n",
        "\t\t\n",
        "\t\treturn action_index, action, current_q\n",
        "\n",
        "\tdef greedy_policy(self, q_values):\n",
        "\t\t\n",
        "\t\taction = np.argmax(q_values)\n",
        "\n",
        "\t\treturn action\n",
        "\t\n",
        "\tdef update_model(self):\n",
        "\t\n",
        "\t\tstates = []\n",
        "\t\tobs_states = []\n",
        "\t\tbatch = self.memory.sample_batch()\n",
        "\n",
        "\t\tif batch == None:\n",
        "\t\t\treturn\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tfor tup in batch:\n",
        "\t\t\t\tstate, action_index, reward, obs, done = tup\n",
        "\t\t\t\tstates.append(state)\n",
        "\t\t\t\tobs_states.append(obs)\n",
        "\t\t\t\n",
        "\t\t\tstates = np.asarray(states)\n",
        "\t\t\tobs_states = np.asarray(obs_states)\n",
        "\n",
        "\t\t\ttarget_q = (self.train_network.model.predict(states))\n",
        "\t\t\ttarget_q_obs = (self.target_network.model.predict(obs_states))\n",
        "\t\t\t\t\n",
        "\t\t\ti = 0\t\n",
        "\t\t\tfor tup in batch:\n",
        "\t\t\t\tstate, action_index, reward, obs, done = tup\n",
        "\t\t\t\tif done:\n",
        "\t\t\t\t\ttarget_q[i][action_index] = reward\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tq_s = max(target_q_obs[i])\n",
        "\t\t\t\t\ttarget_q[i][action_index] = reward + q_s * self.gamma\n",
        "\t\t\t\ti += 1\n",
        "\n",
        "\t\t\tself.train_network.model.train_on_batch(states, target_q) ##### Backprop qvalues\n",
        "\n",
        "\tdef train(self):\n",
        "\t\t\n",
        "\t\tself.test_list = []\n",
        "\t\tself.train_td = []\n",
        "\t\tepisode = 0\n",
        "\t\twhile episode < self.num_episodes:\n",
        "\t\t\tdone = False\n",
        "\t\t\tstate = self.env.reset()\n",
        "\t\t\tepisode_td = []\n",
        "\t\t\tepisode_reward = 0\n",
        "\t\t\twhile not done:\n",
        "\t\t\t\tq_values = (self.train_network.model.predict(state[None, :]))[0]\n",
        "\t\t\t\taction_index, action, q = self.epsilon_greedy_policy(q_values)\n",
        "\t\t\t\tobs, reward, done,  _ = self.env.step(action)\n",
        "\t\t\t\tepisode_reward += reward\n",
        "\t\t\t\t\n",
        "\t\t\t\ttd_step = abs(reward + self.gamma * np.max((self.target_network.model.predict(obs[None, :]))) - q)\n",
        "\t\t\t\tepisode_td.append(td_step)\n",
        "\t\t\t\tself.memory.append((state, action_index, reward, obs, done))\n",
        "\t\t\t\tself.update_model()\n",
        "\t\t\t\tstate = obs\n",
        "\t\t\t\n",
        "\t\t\tself.train_td.append(np.mean(np.array(episode_td)))\n",
        "\n",
        "\t\t\tprint('Episode: ', episode)\n",
        "\t\t\tprint('Current Train Reward: ', episode_reward)\n",
        "\t\t\tprint('Current TD Error: ', self.train_td[-1])\n",
        "\t\t\tprint('\\n')\n",
        "\n",
        "\t\t\tif episode % self.target_update == 0:\n",
        "\t\t\t\tself.target_network.model.set_weights(self.train_network.model.get_weights())\n",
        "\t\t\t\n",
        "\t\t\tif episode > 50:\n",
        "\t\t\t\tself.eps_limit = max(self.eps_limit - 0.005, 0.01) # Define exploration decay rate\n",
        "\t\t\t\n",
        "\t\t\tif episode % 100 == 0:\n",
        "\t\t\t\tself.train_network.save_model_weights()\n",
        "\t\t\t\n",
        "\t\t\tif episode % 100 == 0:\n",
        "\t\t\t\tcurrent_test_reward = self.test(20)\n",
        "\t\t\t\tself.test_list.append(current_test_reward)\n",
        "\t\t\t\tprint('Current Test Reward: ', current_test_reward)\n",
        "\t\t\t\tnp.savetxt('td_error.txt', self.train_td)\n",
        "\t\t\t\tnp.savetxt('test_list.txt', self.test_list)\n",
        "\t\t\t\t\n",
        "\t\t\t\tif current_test_reward >= 10:\n",
        "\t\t\t\t\tprint('Problem solved!')\n",
        "\t\t\t\t\tself.train_network.save_model_weights()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tself.num_episodes = episode + 2\n",
        "\n",
        "\t\t\tepisode += 1\n",
        "\t\t\n",
        "\t\tself.test_reward = self.test_list\n",
        "\t\t\n",
        "\t\tnp.savetxt('td.txt', self.train_td)\n",
        "\t\tnp.savetxt('reward.txt', self.test_reward)\n",
        "\t\tplt.figure(figsize = (12, 8))\n",
        "\t\tplt.plot(np.arange(len(self.test_reward)) * 100, np.ones(len(self.test_reward)) * 200, label = 'Target reward')\n",
        "\t\tplt.plot(np.arange(len(self.test_reward)) * 100, self.test_reward, label = 'Mean reward')\n",
        "\t\tplt.grid()\n",
        "\t\tplt.xlabel('Number of episodes', fontsize = 18)\n",
        "\t\tplt.ylabel('Cumulative reward', fontsize = 18)\n",
        "\t\tplt.legend(fontsize = 18)\n",
        "\t\tplt.show()\n",
        "\t\t\n",
        "\t\tplt.figure(figsize = (12, 8))\n",
        "\t\tplt.plot(np.arange(len(self.train_td)) * 100, self.train_td, label = 'Train TD Error')\n",
        "\t\tplt.grid()\n",
        "\t\tplt.xlabel('Number of Training Episodes', fontsize = 18)\n",
        "\t\tplt.ylabel('Average TD Error', fontsize = 18)\n",
        "\t\tplt.legend(fontsize = 18)\n",
        "\t\tplt.show()\n",
        "\n",
        "\tdef test(self, episodes):\n",
        "\t\t\n",
        "\t\ttest_reward = []\n",
        "\t\tfor i in range(episodes):\n",
        "\t\t\tdone = False\n",
        "\t\t\tstate = self.env.reset()\n",
        "\t\t\tepisode_reward = 0\n",
        "\t\t\twhile not done:\n",
        "\t\t\t\taction_index = np.argmax(self.target_network.model.predict(state[None, :]))\n",
        "\t\t\t\taction = self.index_to_array(action_index)\n",
        "\t\t\t\tobs, reward, done, _ = self.env.step(action)\n",
        "\t\t\t\tepisode_reward += reward\n",
        "\t\t\t\tstate = obs\n",
        "\t\t\ttest_reward.append(episode_reward)\n",
        "\n",
        "\t\treturn np.mean(test_reward)\n",
        "\n",
        "def main():\n",
        "\n",
        "\tMODEL = DQN_Agent(RLTetherAviary(gui = 0, record = 0))\n",
        "\t# MODEL = DQN_Agent(RLCrazyFlieAviary(gui=False, record=False))\n",
        "\tMODEL.train()\n",
        "\t\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Zi5g2VOnJb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}