{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_Plots.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNi8QomHAR/gDW3K5aXU/iM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukeSchmitt96/gym-pybullet-drones/blob/master/tether_sim/DDPG_Plots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh6uP-U8pcyu"
      },
      "source": [
        "!git clone https://github.com/LukeSchmitt96/gym-pybullet-drones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XU_WJWWpm6Z"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsapDGk0poy8"
      },
      "source": [
        "pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjo0JXvzEFIy"
      },
      "source": [
        "pip install 'ray[rllib]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1HSfwubprdN"
      },
      "source": [
        "pip install stable-baselines3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAkoscpspvoB"
      },
      "source": [
        "cd gym-pybullet-drones/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JlgFgSupxVH"
      },
      "source": [
        "pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpUkqkO4K97N"
      },
      "source": [
        "**No problem if sudo apt gives error**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYIyBBzWES-_"
      },
      "source": [
        "sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb8a9TpBEVtm"
      },
      "source": [
        "pip install stable-baselines[mpi]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-01QDdPLEX5-"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMovt8-bKujQ"
      },
      "source": [
        "**Make sure TensorFLow is switched to 1.x before running next cell**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e56K9xEOEaIh"
      },
      "source": [
        "import stable_baselines\n",
        "stable_baselines.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3dp_IXapyyQ"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import pdb\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet as p\n",
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines3.ddpg.policies import MlpPolicy\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "# import tensorflow.contrib.tensorrt as trt\n",
        "\n",
        "from stable_baselines import results_plotter\n",
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n",
        "# from stable_baselines3.common.noise import AdaptiveParamNoiseSpec\n",
        "from stable_baselines.common.callbacks import BaseCallback\n",
        "\n",
        "from gym_pybullet_drones.envs.RLTetherAviary import RLTetherAviary"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmOn8PnqqDVM"
      },
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
        "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA-IIRvZEkys"
      },
      "source": [
        "# Create log dir\n",
        "log_dir = \"tmp/\"\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-nwprrJNkM"
      },
      "source": [
        "**Check GUI Record before running next cell**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPwFpAniEvJS"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #### Check the environment's spaces ################################################################\n",
        "    #env = gym.make(\"rl-CrazyFlie-aviary-v0\")\n",
        "    env = RLTetherAviary(gui=0, record=0)\n",
        "    env = Monitor(env, log_dir)\n",
        "    print(\"[INFO] Action space:\", env.action_space)\n",
        "    print(\"[INFO] Observation space:\", env.observation_space)\n",
        "    # print(\"[INFO] Checking Environment...\")\n",
        "    # check_env(env, warn=True, skip_render_check=True) \n",
        "\n",
        "    #### Train the model ###############################################################################\n",
        "    n_actions = env.action_space.shape[-1]\n",
        "    #action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions), dt = 0.005)\n",
        "    callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
        "    model = DDPG(MlpPolicy, env, verbose=1, batch_size=64, action_noise=action_noise)\n",
        "\n",
        "    training_timesteps = 200000\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        # print(\"Iteration\\t\", i)\n",
        "\n",
        "        model.learn(total_timesteps=training_timesteps)\n",
        "        model.save(\"ddpg\"+str((i+1)*training_timesteps))\n",
        "        model.save_replay_buffer(\"ddpg_experience\"+str((i+1)*training_timesteps))\n",
        "        results_plotter.plot_results([log_dir], training_timesteps, results_plotter.X_TIMESTEPS, \"DDPG Tethered Quadcopter\")\n",
        "        plt.show()\n",
        "\n",
        "        #### Show (and record a video of) the model's performance ##########################################\n",
        "        env_test = RLTetherAviary(gui=0, record=0)\n",
        "        obs = env_test.reset()\n",
        "        start = time.time()\n",
        "        for i in range(10*env_test.SIM_FREQ):\n",
        "            action, _states = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, info = env_test.step(action)\n",
        "            print(i)\n",
        "            print(obs)\n",
        "            print(done)\n",
        "            env_test.render()\n",
        "            if done: break\n",
        "        env_test.close()\n",
        "\n",
        "    env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLnkSfJHG88w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}