{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_FR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv5wz+Yn3fRI9o2e3yTUJK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukeSchmitt96/gym-pybullet-drones/blob/master/tether_sim/DQN_FR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0-tAyNtaDKx"
      },
      "source": [
        "!git clone https://github.com/LukeSchmitt96/gym-pybullet-drones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiFCO-0mhLxx"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VJpD3--hNM8"
      },
      "source": [
        "pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmTUpgs1hO4O"
      },
      "source": [
        "pip install stable-baselines3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejpBqHWYhQXO"
      },
      "source": [
        "pip install 'ray[rllib]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqekwLiqhRpH"
      },
      "source": [
        "cd gym-pybullet-drones/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF_0HhvFhT13"
      },
      "source": [
        "pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpoDn8_AhE68"
      },
      "source": [
        "#!/usr/bin/env python\r\n",
        "\r\n",
        "import keras, tensorflow as tf, numpy as np, gym, sys, copy, argparse\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.models import model_from_json\r\n",
        "import collections\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from gym_pybullet_drones.envs.RLTetherAviary import RLTetherAviary\r\n",
        "# from gym_pybullet_drones.envs.RLCrazyFlieAviary import RLCrazyFlieAviary\r\n",
        "\r\n",
        "class DQN():\r\n",
        "\r\n",
        "\tdef __init__(self, environment):\r\n",
        "\t\t\r\n",
        "\t\tself.environment = environment\r\n",
        "\t\tself.num_actions = self.environment.action_space.shape[0] # Pitch forward, Pitch backward, Roll left, Roll right, Hover\r\n",
        "\t\tself.state_size = self.environment.reset().shape[0]\r\n",
        "\t\tself.model = Sequential()\r\n",
        "\t\tself.model.add(Dense(64, input_dim=self.state_size, activation = 'relu')) # Define number of layers, neurons and activation\r\n",
        "\t\tself.model.add(Dense(64, activation = 'relu')) # Define number of layers, neurons and activation\r\n",
        "\t\tself.model.add(Dense(32, activation = 'relu')) # Define number of layers, neurons and activation\r\n",
        "\t\tself.model.add(Dense(self.num_actions))\r\n",
        "\t\tself.model.compile(loss = 'mse', optimizer = Adam(lr = 0.001)) # Define loss and learning rate\r\n",
        "\r\n",
        "\tdef save_model_weights(self):\r\n",
        "\t\t\r\n",
        "\t\tmodel_json = self.model.to_json()\r\n",
        "\t\twith open(\"model.json\", \"w\") as json_file:\r\n",
        "\t\t\tjson_file.write(model_json)\r\n",
        "\t\tself.model.save_weights(\"model.h5\")\r\n",
        "\r\n",
        "\tdef load_model(self, model_file):\r\n",
        "\t\t\r\n",
        "\t\tjson_file = open('model.json', 'r')\r\n",
        "\t\tloaded_model_json = json_file.read()\r\n",
        "\t\tjson_file.close()\r\n",
        "\t\tself.model = model_from_json(loaded_model_json)\r\n",
        "\r\n",
        "\tdef load_model_weights(self, weight_file):\r\n",
        "\t\t\r\n",
        "\t\tself.model.load_weights(\"model.h5\")\r\n",
        "\r\n",
        "\r\n",
        "class Replay_Memory():\r\n",
        "\r\n",
        "\tdef __init__(self, memory_size = 50000): # Define replay memory size\r\n",
        "\t\t\r\n",
        "\t \tself.memory = collections.deque(maxlen = memory_size)\r\n",
        "\r\n",
        "\tdef sample_batch(self, batch_size = 64): # Define batch size\r\n",
        "\t\t\r\n",
        "\t\tif len(self.memory) < batch_size:\r\n",
        "\t\t\treturn None\r\n",
        "\t\telse:\r\n",
        "\t\t\treturn random.sample(self.memory, batch_size)\r\n",
        "\r\n",
        "\tdef append(self, transition):\r\n",
        "\t\t\r\n",
        "\t\tself.memory.append(transition)\r\n",
        "\r\n",
        "class DQN_Agent():\r\n",
        "\r\n",
        "\tdef __init__(self, environment_name):\r\n",
        "\r\n",
        "\t\t# self.environment_name = environment_name\r\n",
        "\t\tself.env = environment_name # gym.envs.make(self.environment_name)\r\n",
        "\t\tself.train_network = DQN(self.env)\r\n",
        "\t\tself.target_network = DQN(self.env)\r\n",
        "\t\tself.target_network.model.set_weights(self.train_network.model.get_weights())\r\n",
        "\t\tself.memory = Replay_Memory()\r\n",
        "\t\tself.num_episodes = 1500 # Define number of episodes\r\n",
        "\t\tself.gamma = 0.99 # Define discount rate\r\n",
        "\t\tself.eps_limit = 0.5 # Define Epsilon\r\n",
        "\t\tself.target_update = 5 # Define target net update frequency\r\n",
        "\t\tself.max_tether_force = self.env.MAX_TETHER_FORCE\r\n",
        "\t\tself.tether_length = self.env.TETHER_MIN_LENGTH\r\n",
        "\r\n",
        "\tdef index_to_array(self, index, state):\r\n",
        "\r\n",
        "\t\t# Hover RPM = 14468 Max RPM = 21702 Tether Length = 0.2 Max Tether Force = 0.7144\r\n",
        "\t\t# Hover / Max = 0.66\r\n",
        "\t\tdistance = np.linalg.norm(state[0:3])\r\n",
        "\t\tif distance > self.tether_length:\r\n",
        "\t\t\ttether_force = (np.absolute(distance - self.tether_length) / distance) # Define max tether force\r\n",
        "\t\telse:\r\n",
        "\t\t\ttether_force = 0 \r\n",
        "\r\n",
        "\t\t# Set 1\r\n",
        "\t\tmin = 0.62 # Define RPM\r\n",
        "\t\tmax = 0.70 # Define RPM\r\n",
        "\r\n",
        "\t\t# Set 2\r\n",
        "\t\t# min = 0.52 # Define RPM\r\n",
        "\t\t# max = 0.80 # Define RPM\r\n",
        "\r\n",
        "\t\t# Set 3\r\n",
        "\t\t# min = 0.42 # Define RPM\r\n",
        "\t\t# max = 0.90 # Define RPM\r\n",
        "\t\t\r\n",
        "\t\thov = 0 # Define Hover RPM\r\n",
        "\r\n",
        "\t\tif index == 0:\r\n",
        "\t\t\taction_array = np.array([min, min, max, max, tether_force]) # Rotor 3 and Rotor 4, X-axis\r\n",
        "\t\telif index == 1:\r\n",
        "\t\t\taction_array = np.array([max, max, min, min, tether_force]) # Rotor 1 and Rotor 2, X-axis\r\n",
        "\t\telif index == 2:\r\n",
        "\t\t\taction_array = np.array([max, min, max, min, tether_force]) # Rotor 1 and Rotor 3, Y-axis\r\n",
        "\t\telif index == 3:\r\n",
        "\t\t\taction_array = np.array([min, max, min, max, tether_force]) # Rotor 2 and Rotor 4, Y-axis\r\n",
        "\t\telif index == 4:\r\n",
        "\t\t\taction_array = np.array([hov, hov, hov, hov, tether_force]) # Hover\r\n",
        "\t\t\r\n",
        "\t\treturn action_array\r\n",
        "\r\n",
        "\tdef epsilon_greedy_policy(self, q_values,state):\r\n",
        "\t\t\r\n",
        "\t\teps = np.random.random(1)[0]\r\n",
        "\t\tif eps < self.eps_limit:\r\n",
        "\t\t\taction_index = np.random.randint(0, self.train_network.num_actions)\r\n",
        "\t\r\n",
        "\t\telse:\r\n",
        "\t\t\taction_index = np.argmax(q_values)\r\n",
        "\t\t\r\n",
        "\t\taction = self.index_to_array(action_index, state)\r\n",
        "\r\n",
        "\t\tcurrent_q = q_values[action_index]\r\n",
        "\t\t\r\n",
        "\t\treturn action_index, action, current_q\r\n",
        "\r\n",
        "\tdef greedy_policy(self, q_values):\r\n",
        "\t\t\r\n",
        "\t\taction = np.argmax(q_values)\r\n",
        "\r\n",
        "\t\treturn action\r\n",
        "\t\r\n",
        "\tdef update_model(self):\r\n",
        "\t\r\n",
        "\t\tstates = []\r\n",
        "\t\tobs_states = []\r\n",
        "\t\tbatch = self.memory.sample_batch()\r\n",
        "\r\n",
        "\t\tif batch == None:\r\n",
        "\t\t\treturn\r\n",
        "\t\t\r\n",
        "\t\telse:\r\n",
        "\t\t\tfor tup in batch:\r\n",
        "\t\t\t\tstate, action_index, reward, obs, done = tup\r\n",
        "\t\t\t\tstates.append(state)\r\n",
        "\t\t\t\tobs_states.append(obs)\r\n",
        "\t\t\t\r\n",
        "\t\t\tstates = np.asarray(states)\r\n",
        "\t\t\tobs_states = np.asarray(obs_states)\r\n",
        "\r\n",
        "\t\t\ttarget_q = (self.train_network.model.predict(states))\r\n",
        "\t\t\ttarget_q_obs = (self.target_network.model.predict(obs_states))\r\n",
        "\t\t\t\t\r\n",
        "\t\t\ti = 0\t\r\n",
        "\t\t\tfor tup in batch:\r\n",
        "\t\t\t\tstate, action_index, reward, obs, done = tup\r\n",
        "\t\t\t\tif done:\r\n",
        "\t\t\t\t\ttarget_q[i][action_index] = reward\r\n",
        "\t\t\t\telse:\r\n",
        "\t\t\t\t\tq_s = max(target_q_obs[i])\r\n",
        "\t\t\t\t\ttarget_q[i][action_index] = reward + q_s * self.gamma\r\n",
        "\t\t\t\ti += 1\r\n",
        "\r\n",
        "\t\t\tself.train_network.model.train_on_batch(states, target_q) ##### Backprop qvalues\r\n",
        "\r\n",
        "\tdef train(self):\r\n",
        "\t\t\r\n",
        "\t\tself.test_list = []\r\n",
        "\t\tself.train_td = []\r\n",
        "\t\tself.reward_plot = []\r\n",
        "\t\tepisode = 0\r\n",
        "\t\twhile episode < self.num_episodes:\r\n",
        "\t\t\tdone = False\r\n",
        "\t\t\tstate = self.env.reset()\r\n",
        "\t\t\t# print('Reset State', state, 'Shape', np.size(state))\r\n",
        "\t\t\tepisode_td = []\r\n",
        "\t\t\tepisode_reward = 0\r\n",
        "\t\t\twhile not done:\r\n",
        "\t\t\t\tq_values = (self.train_network.model.predict(state[None, :]))[0]\r\n",
        "\t\t\t\taction_index, action, q = self.epsilon_greedy_policy(q_values, state)\r\n",
        "\t\t\t\tobs, reward, done,  _ = self.env.step(action)\r\n",
        "\t\t\t\tepisode_reward += reward\r\n",
        "\t\t\t\t\r\n",
        "\t\t\t\ttd_step = abs(reward + self.gamma * np.max((self.target_network.model.predict(obs[None, :]))) - q)\r\n",
        "\t\t\t\tepisode_td.append(td_step)\r\n",
        "\t\t\t\tself.memory.append((state, action_index, reward, obs, done))\r\n",
        "\t\t\t\tself.update_model()\r\n",
        "\t\t\t\tstate = obs\r\n",
        "\r\n",
        "\t\t\tself.train_td.append(np.mean(np.array(episode_td)))\r\n",
        "\t\t\tself.reward_plot.append(episode_reward)\r\n",
        "\r\n",
        "\t\t\tprint('Episode: ', episode)\r\n",
        "\t\t\tprint('Current Train Reward: ', episode_reward)\r\n",
        "\t\t\tprint('Current TD Error: ', self.train_td[-1])\r\n",
        "\t\t\tprint('\\n')\r\n",
        "\r\n",
        "\t\t\tif episode % self.target_update == 0:\r\n",
        "\t\t\t\tself.target_network.model.set_weights(self.train_network.model.get_weights())\r\n",
        "\t\t\t\r\n",
        "\t\t\tif episode > 50:\r\n",
        "\t\t\t\tself.eps_limit = max(self.eps_limit - 0.005, 0.01) # Define exploration decay rate\r\n",
        "\t\t\t\r\n",
        "\t\t\tif episode % 100 == 0: \r\n",
        "\t\t\t\tself.train_network.save_model_weights()\r\n",
        "\t\t\t\r\n",
        "\t\t\tif episode % 100 == 0: \r\n",
        "\t\t\t\tcurrent_test_reward = self.test(20)\r\n",
        "\t\t\t\tself.test_list.append(current_test_reward)\r\n",
        "\t\t\t\tprint('Current Test Reward: ', current_test_reward)\r\n",
        "\t\t\t\tnp.savetxt('td_error.txt', self.train_td)\r\n",
        "\t\t\t\tnp.savetxt('test_list.txt', self.test_list)\r\n",
        "\t\t\t\t\r\n",
        "\t\t\t\tif current_test_reward >= 10000: \r\n",
        "\t\t\t\t\tprint('Problem solved!')\r\n",
        "\t\t\t\t\tself.train_network.save_model_weights()\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\t\tself.num_episodes = episode + 2\r\n",
        "\r\n",
        "\t\t\tepisode += 1\r\n",
        "\t\t\r\n",
        "\t\tself.test_reward = self.test_list\r\n",
        "\t\t\r\n",
        "\t\tnp.savetxt('td.txt', self.train_td)\r\n",
        "\t\tnp.savetxt('reward.txt', self.test_reward)\r\n",
        "\t\tplt.figure(figsize = (12, 8))\r\n",
        "\t\tplt.plot(np.arange(len(self.test_reward)) * 100, np.ones(len(self.test_reward)) * 200, label = 'Target reward')\r\n",
        "\t\tplt.plot(np.arange(len(self.test_reward)) * 100, self.test_reward, label = 'Mean reward')\r\n",
        "\t\tplt.grid()\r\n",
        "\t\tplt.xlabel('Number of episodes', fontsize = 18)\r\n",
        "\t\tplt.ylabel('Cumulative reward', fontsize = 18)\r\n",
        "\t\tplt.legend(fontsize = 18)\r\n",
        "\t\tplt.show()\r\n",
        "\t\t\r\n",
        "\t\tplt.figure(figsize = (12, 8))\r\n",
        "\t\tplt.plot(np.arange(len(self.train_td)) * 100, self.train_td, label = 'Train TD Error')\r\n",
        "\t\tplt.grid()\r\n",
        "\t\tplt.xlabel('Number of Training Episodes', fontsize = 18)\r\n",
        "\t\tplt.ylabel('Average TD Error', fontsize = 18)\r\n",
        "\t\tplt.legend(fontsize = 18)\r\n",
        "\t\tplt.show()\r\n",
        "\t\r\n",
        "\t\tplt.figure(figsize = (12, 8))\r\n",
        "\t\tplt.plot(self.reward_plot, label = 'Train TD Error')\r\n",
        "\t\tplt.xlabel('Number of Training Episodes', fontsize = 18)\r\n",
        "\t\tplt.ylabel('Episode Rewards', fontsize = 18)\r\n",
        "\t\tplt.show()\r\n",
        "\r\n",
        "\tdef test(self, episodes):\r\n",
        "\t\t\r\n",
        "\t\ttest_reward = []\r\n",
        "\t\tfor i in range(episodes):\r\n",
        "\t\t\tdone = False\r\n",
        "\t\t\tstate = self.env.reset()\r\n",
        "\t\t\tepisode_reward = 0\r\n",
        "\t\t\twhile not done:\r\n",
        "\t\t\t\taction_index = np.argmax(self.target_network.model.predict(state[None, :]))\r\n",
        "\t\t\t\tprint(action_index)\r\n",
        "\t\t\t\taction = self.index_to_array(action_index, state)\r\n",
        "\t\t\t\tobs, reward, done, _ = self.env.step(action)\r\n",
        "\t\t\t\tepisode_reward += reward\r\n",
        "\t\t\t\tstate = obs\r\n",
        "\t\t\ttest_reward.append(episode_reward)\r\n",
        "\r\n",
        "\t\treturn np.mean(test_reward)\r\n",
        "\r\n",
        "def main():\r\n",
        "\r\n",
        "\tMODEL = DQN_Agent(RLTetherAviary(gui = 0, record = 0))\r\n",
        "\t# MODEL.train_network.load_model('model.json')\r\n",
        "\t# MODEL.train_network.load_model_weights('model.h5')\r\n",
        "\t# MODEL.train_network.model.compile(loss = 'mse', optimizer = Adam(lr = 0.001))\r\n",
        "\tMODEL.train()\r\n",
        "\t\r\n",
        "if __name__ == '__main__':\r\n",
        "\tmain()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}